{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copyright 2020 Enea Dodi\n",
    "https://www.apache.org/licenses/LICENSE-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Lets important everything and not worry for later :)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from click._compat import iteritems\n",
    "import requests\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import threading \n",
    "import collections\n",
    "from pandas.tests.frame.test_sort_values_level_as_str import ascending\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project 1: Stock Market Predictors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### By *Enea Dodi* Summer 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "I have designed a 'course' to learn the fundamentals of Machine Learning:\n",
    "\n",
    "1. Complete Google Machine Learning Crash Course \n",
    "2. Simple/fun ML project to have hands on experience\n",
    "3. Andrew Trask's Grokking Deep Learning\n",
    "4. More sophisticated ML project\n",
    "\n",
    "   **option 1**  \n",
    "   5. Pedro Domingo college course for Machine Learning (found on Youtube)  \n",
    "   6. Finish all HW assignments of this course\n",
    "\n",
    "   **option 2**  \n",
    "   5. Aurélien Géron Hand's On Machine Learning edition 2  \n",
    "   6. Complete plenty of exercises in this book.\n",
    "\n",
    "This project is the second on the list: *A simple/fun ML project to have hands on experience*\n",
    "\n",
    "The use of Artifical Intelligence for Stock Market Analysis/Prediction is a very big and competitive field. There are resources and websites which act as communities and API for developing your own algorithms and papers (such as [quandl](https://www.quandl.com/) and [quantopian](https://www.quantopian.com/)) however **my goal for this project is not to develop a industry standard algorithm, but rather have hands on experience with Machine Learning and the tools involved**\n",
    "\n",
    "I will be using TensorFlow, Pandas, Numpy, and Matplotlib for the development of the algorithm, as well as BeautifulSoup to scrape valuable information from stock analysis websites (such as [finviz](https://finviz.com/)). \n",
    "**Big** thanks to Nicolas P. Rougier for [100 Numpy Exercises](https://github.com/rougier/numpy-100) and Alex Riley for [100 Pandas Exercises](https://github.com/ajcr/100-pandas-puzzles) which acted as tutorials for using Numpy and Pandas correctly for Machine Learning Projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface\n",
    "I would like to establish goals, framing, and overall training route before creation of the Machine Learning algorithms for this project. As a template, I will be using the *Deciding on Machine Learning* outline from Google's Introduction to ML Problem Framing.\n",
    "\n",
    "**1) What should the ML model do?**\n",
    "    - The overall goal of the ML models will be to develop a general classificiation, 'Going up next week' or 'Not going up next week' which works on any Stock Ticker belonging to different sectors, industries, price ranges, and volume \n",
    "    range.\n",
    "**2) What is the ideal outcome?**\n",
    "    - The ideal outcome is to generate a list of stocks which have a 'high probability' of going up in price the \n",
    "    following week so an individual can assess and determine which of the high probability stocks to invest in.\n",
    "**3) How will I know if system is successful or a failure (Sucess / Failure Metrics)**\n",
    "    - Measuring the metrics of success is very simple with stock market predictors. During training and testing, the model will be evaluated on the precision of its classifications for the upcoming week. I will thus know immeditaely, or the following week (as I may decide to evaluate performance on real predictions rather than past price points)\n",
    "    - I will be focusing on precision rather than recall or accuracy. The outcome of investing X amount at 5 companies which all go up 10% is the same as investing X amoumt at 500 companies which all go up 10%. Thus a **low false positive rate** is more important than a **low false negative rate**.\n",
    "    Of course this premise can be challenged. For example if we are investing X amount at 5 companies chosen through a precision omptimized metric, then incorrect labeling, no matter how small of a chance, will result in a larger chunk of cash lost per error compared to investing X amount at 500 companies chosen through a recall optimized metric. Thus I will not attempt to minmax for any strategy, but rather, optimize for precision so far as the batch of positive classifications remains in the low dozens (out of the 1500+ Tickers evaluated). In other words, I will attempt to optimize the precision such as: \n",
    "$$(FPR) * [\\frac{X}{PCC}]  \\leq  [\\frac{X}{TPC}] $$\n",
    "\n",
    "     Where FPR = False Positive Rate ; X = Total Invested Money ; PCC = Positive Clasified Cases ; TPC = True Postiive Cases\n",
    "    \n",
    "    - I will also be displaying the Confusion Matrices, ROC curves, and AUC curves on evaluations. \n",
    "    \n",
    "    \n",
    "    - Finally, stock prices are actually able to do three things: decrease in price, increase in price, or stay the same. The classifier on the other hand only classifies between 'Going up next week' or 'Not going up next week'. Thus I will decide in mostly a subjective matter how to divide up the three classes. \n",
    "        - 'Going up next week' or the positive label will be those stocks which demostrate a 3% or more increase in price in the following week.\n",
    "        - 'Not going up next week' or the negative label will be those stocks which do not demostrate a 3% or more increase in price the following week.\n",
    "            - Upon further development of the models, I may replace the '3%' with the average variance in price of the\n",
    "            specified stock.\n",
    "            \n",
    "**4) Are the Metrics Measurable?**\n",
    "    - All the specified metrics mentioned above are measurable through Stock Market and Data Analysis as they are objective. \n",
    "**5) What failure scenarios are not related to the sucess Matrix?**\n",
    "    - Of course, the features I will be feeding the algorithms are limited. The model must be refurbished frequently as\n",
    "    features such as 'Institutional Holders', 'Income' , 'Sales', and 'Recommendations' are all volatile. Also, the model\n",
    "    is not immune to concept drift, political influence, and unforseen world circumstances. Thus, entire sectors for\n",
    "    example may fall due to some influence not listed on the features and thus these scenarios should not be counted\n",
    "    against the algorithm.\n",
    "    - As a third of the data pushed as features take place during a global pandemic, the model may learn certain \n",
    "    attributes that applying well to the current world circumstances but not towards a 'regular' or 'normal' world\n",
    "    circumstances(which the concepts of may or may not exist as the world is constantly in flux)\n",
    "**6) How will the product use the predictions**\n",
    "     - These positive classifications will then optimally be filtered down through a human interpreter to a hand-full of \n",
    "    Tickers which then the human interpreter can decide to invest in.\n",
    "**7) Where in the architecture should the code live?**\n",
    "    - Every week I shall scrape the closing values on a Thursday and the model should be able to classify which Tickers \n",
    "    will 'Go up next week' before 4pm EST on Friday. There is ample time in this gap, the scraping will be\n",
    "    automated and will only take a small fraction of the day, and the evaluation even less time. Thus there are no\n",
    "    latency requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As restrictions I faced with the development of this project included cost, I sought out websites and APIs that were of no cost. While Quandl/Quantopian provides important and consistent data on stocks, free users face many restrictions. Thus I decided to use two sources for data extraction:\n",
    "### 1) Finviz\n",
    "         Finviz is a stock screener and financial visualizer with an awesome free option. It includes a large host\n",
    "         of analytical information (such as Quick Ratio, Average True Range, Short Float, Short Ratio, \n",
    "         Insider Own, etc) as well as a candleled chart with some pattern recogniziton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Finviz_Graph.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Finviz_table.png\"  width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Yahoo! Finance\n",
    "    Yahoo! finance provides financial news, data, and commentary for the stock market. Yahoo! Primarily I \n",
    "    interacted with Yahoo! Finance through [YFinance](https://pypi.org/project/yfinance/) as Yahoo! decommisioned\n",
    "    their historical data API a couple years back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Yahoo_F_L.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finviz offers a large amount of information in the table that could be useful for the ML algorithm to make predictions. However, **A LOT** of this information will offer bias due to them being calculated using performance in the most recent time frame (One week, One month, etc). Thus I will be searching for mostly static values. Ontop of that, unlike the table demostrated above, for the majority of stocks, a lot of information is missing. As an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Avg_table.png\"  width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I face restrictions on the number of features that should be used on my model. Of course, the fewer the better. Finally, some really good potential features, such as RSI and ATR are momentum indicators and derive from the preivous 14 day values. If I had decided on rescraping/rebuilding my model every day, I would surely use these features. However the purpose of this project is to get my hands dirty in Machine Learning, Data Scraping, and Data Preperation. Thus this will not be necassary for my model. Of course, my model is to be used as a 'filtration' system to recommend the best stocks to look at for a trader. Thus features such as RSI,ATR, and even Short Ratio **should** be used by the actual trader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these restrictions, I had to choose a features that prioritized staticity and were found perfeably on every ticker listed on Finviz. I decided on:\n",
    "   #####  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sector, Industry, Country, Recommendations, Income, and Sales   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YFinance will be used to get the historical chart data for each of the stocks chosen from Finviz. Again, as no specialist, I decided to not rely on Opening, High, Low values. Rather I am interested in how the stock's price looks at **Closing**. However there is a lot of volatility day to day, and this may serve to confuse the model. I am also not interested in short term trades. \n",
    "\n",
    "I thought the larger the time interval per data point, the more consistent to overall trends the price and volume will be. Thus I decided to include in my 'features' (the quotations will make sense towards the end of the project):  \n",
    "#####  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2-Year weekly Closing and Volume values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data Extraction Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
