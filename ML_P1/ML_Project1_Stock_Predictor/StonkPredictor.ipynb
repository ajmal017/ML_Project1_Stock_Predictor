{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copyright 2020 Enea Dodi\n",
    "https://www.apache.org/licenses/LICENSE-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Lets important everything and not worry for later :)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from click._compat import iteritems\n",
    "import requests\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import threading \n",
    "import collections\n",
    "from pandas.tests.frame.test_sort_values_level_as_str import ascending\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project 1: Stock Market Predictors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### By *Enea Dodi* Summer 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "I have designed a 'course' to learn the fundamentals of Machine Learning:\n",
    "\n",
    "1. Complete Google Machine Learning Crash Course \n",
    "2. Simple/fun ML project to have hands on experience\n",
    "3. Andrew Trask's Grokking Deep Learning\n",
    "4. More sophisticated ML project\n",
    "\n",
    "   **option 1**  \n",
    "   5. Pedro Domingo college course for Machine Learning (found on Youtube)  \n",
    "   6. Finish all HW assignments of this course\n",
    "\n",
    "   **option 2**  \n",
    "   5. Aurélien Géron Hand's On Machine Learning edition 2  \n",
    "   6. Complete plenty of exercises in this book.\n",
    "\n",
    "This project is the second on the list: *A simple/fun ML project to have hands on experience*\n",
    "\n",
    "The use of Artifical Intelligence for Stock Market Analysis/Prediction is a very big and competitive field. There are resources and websites which act as communities and API for developing your own algorithms and papers (such as [quandl](https://www.quandl.com/) and [quantopian](https://www.quantopian.com/)) however **my goal for this project is not to develop a industry standard algorithm, but rather have hands on experience with Machine Learning and the tools involved**\n",
    "\n",
    "I will be using TensorFlow, Pandas, Numpy, and Matplotlib for the development of the algorithm, as well as BeautifulSoup to scrape valuable information from stock analysis websites (such as [finviz](https://finviz.com/)). \n",
    "**Big** thanks to Nicolas P. Rougier for [100 Numpy Exercises](https://github.com/rougier/numpy-100) and Alex Riley for [100 Pandas Exercises](https://github.com/ajcr/100-pandas-puzzles) which acted as tutorials for using Numpy and Pandas correctly for Machine Learning Projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface\n",
    "I would like to establish goals, framing, and overall training route before creation of the Machine Learning algorithms for this project. As a template, I will be using the *Deciding on Machine Learning* outline from Google's Introduction to ML Problem Framing.\n",
    "\n",
    "**1) What should the ML model do?**\n",
    "    - The overall goal of the ML models will be to develop a general classificiation, 'Going up next week' or 'Not going up next week' which works on any Stock Ticker belonging to different sectors, industries, price ranges, and volume \n",
    "    range.\n",
    "**2) What is the ideal outcome?**\n",
    "    - The ideal outcome is to generate a list of stocks which have a 'high probability' of going up in price the \n",
    "    following week so an individual can assess and determine which of the high probability stocks to invest in.\n",
    "**3) How will I know if system is successful or a failure (Sucess / Failure Metrics)**\n",
    "    - Measuring the metrics of success is very simple with stock market predictors. During training and testing, the model will be evaluated on the precision of its classifications for the upcoming week. I will thus know immeditaely, or the following week (as I may decide to evaluate performance on real predictions rather than past price points)\n",
    "    - I will be focusing on precision rather than recall or accuracy. The outcome of investing X amount at 5 companies which all go up 10% is the same as investing X amoumt at 500 companies which all go up 10%. Thus a **low false positive rate** is more important than a **low false negative rate**.\n",
    "    Of course this premise can be challenged. For example if we are investing X amount at 5 companies chosen through a precision omptimized metric, then incorrect labeling, no matter how small of a chance, will result in a larger chunk of cash lost per error compared to investing X amount at 500 companies chosen through a recall optimized metric. Thus I will not attempt to minmax for any strategy, but rather, optimize for precision so far as the batch of positive classifications remains in the low dozens (out of the 1500+ Tickers evaluated). In other words, I will attempt to optimize the precision such as: \n",
    "$$(FPR) * [\\frac{X}{PCC}]  \\leq  [\\frac{X}{TPC}] $$\n",
    "\n",
    "     Where FPR = False Positive Rate ; X = Total Invested Money ; PCC = Positive Clasified Cases ; TPC = True Postiive Cases\n",
    "    \n",
    "    - I will also be displaying the Confusion Matrices, ROC curves, and AUC curves on evaluations. \n",
    "    \n",
    "    \n",
    "    - Finally, stock prices are actually able to do three things: decrease in price, increase in price, or stay the same. The classifier on the other hand only classifies between 'Going up next week' or 'Not going up next week'. Thus I will decide in mostly a subjective matter how to divide up the three classes. \n",
    "        - 'Going up next week' or the positive label will be those stocks which demostrate a 3% or more increase in price in the following week.\n",
    "        - 'Not going up next week' or the negative label will be those stocks which do not demostrate a 3% or more increase in price the following week.\n",
    "            - Upon further development of the models, I may replace the '3%' with the average variance in price of the\n",
    "            specified stock.\n",
    "            \n",
    "**4) Are the Metrics Measurable?**\n",
    "    - All the specified metrics mentioned above are measurable through Stock Market and Data Analysis as they are objective. \n",
    "**5) What failure scenarios are not related to the sucess Matrix?**\n",
    "    - Of course, the features I will be feeding the algorithms are limited. The model must be refurbished frequently as\n",
    "    features such as 'Institutional Holders', 'Income' , 'Sales', and 'Recommendations' are all volatile. Also, the model\n",
    "    is not immune to concept drift, political influence, and unforseen world circumstances. Thus, entire sectors for\n",
    "    example may fall due to some influence not listed on the features and thus these scenarios should not be counted\n",
    "    against the algorithm.\n",
    "    - As a third of the data pushed as features take place during a global pandemic, the model may learn certain \n",
    "    attributes that applying well to the current world circumstances but not towards a 'regular' or 'normal' world\n",
    "    circumstances(which the concepts of may or may not exist as the world is constantly in flux)\n",
    "**6) How will the product use the predictions**\n",
    "     - These positive classifications will then optimally be filtered down through a human interpreter to a hand-full of \n",
    "    Tickers which then the human interpreter can decide to invest in.\n",
    "**7) Where in the architecture should the code live?**\n",
    "    - Every week I shall scrape the closing values on a Thursday and the model should be able to classify which Tickers \n",
    "    will 'Go up next week' before 4pm EST on Friday. There is ample time in this gap, the scraping will be\n",
    "    automated and will only take a small fraction of the day, and the evaluation even less time. Thus there are no\n",
    "    latency requirements\n",
    "\n",
    "### Finally,\n",
    "all my code can be found in the repository. Besides the building of the ML algorithm (which will vastly happen here), all code for Data Extraction and Preperation can be found under the src directory. As there are over a thousand lines of code, it would make no sense to copy paste it all on the presentation. Thus, I will be simply showcasing a few important methods, with **omitted comments** on the Jupyter notebook and the rest of my code can be viewed seperately :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As restrictions I faced with the development of this project included cost, I sought out websites and APIs that were of no cost. While Quandl/Quantopian provides important and consistent data on stocks, free users face many restrictions. Thus I decided to use two sources for data extraction:\n",
    "\n",
    "**NOTE** I was very hyped for the Data Extraction and Preperation. On the Google Machine Learning Crash Course, they stated that the majority of time (some say 80%+) is spend on extraction and preparation of data for Machine Learning models. Thus I wanted to see how I handled that phase. TL;DR I didn't mind it at all!! \n",
    "### 1) Finviz\n",
    "         Finviz is a stock screener and financial visualizer with an awesome free option. It includes a large host\n",
    "         of analytical information (such as Quick Ratio, Average True Range, Short Float, Short Ratio, \n",
    "         Insider Own, etc) as well as a candleled chart with some pattern recogniziton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Finviz_Graph.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Finviz_table.png\"  width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Yahoo! Finance\n",
    "    Yahoo! finance provides financial news, data, and commentary for the stock market. Yahoo! Primarily I \n",
    "    interacted with Yahoo! Finance through [YFinance](https://pypi.org/project/yfinance/) as Yahoo! decommisioned\n",
    "    their historical data API a couple years back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Yahoo_F_L.png\"  width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finviz offers a large amount of information in the table that could be useful for the ML algorithm to make predictions. However, **A LOT** of this information will offer bias due to them being calculated using performance in the most recent time frame (One week, One month, etc). Thus I will be searching for mostly static values. Ontop of that, unlike the table demostrated above, for the majority of stocks, a lot of information is missing. As an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Avg_table.png\"  width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I face restrictions on the number of features that should be used on my model. Of course, the fewer the better. Finally, some really good potential features, such as RSI and ATR are momentum indicators and derive from the preivous 14 day values. If I had decided on rescraping/rebuilding my model every day, I would surely use these features. However the purpose of this project is to get my hands dirty in Machine Learning, Data Scraping, and Data Preperation. Thus this will not be necassary for my model. Of course, my model is to be used as a 'filtration' system to recommend the best stocks to look at for a trader. Thus features such as RSI,ATR, and even Short Ratio **should** be used by the actual trader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these restrictions, I had to choose a features that prioritized staticity and were found perfeably on every ticker listed on Finviz. I decided on:\n",
    "   #####  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sector, Industry, Country, Recommendations, Income, and Sales   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YFinance will be used to get the historical chart data for each of the stocks chosen from Finviz. Again, as no specialist, I decided to not rely on Opening, High, Low values. Rather I am interested in how the stock's price looks at **Closing**. However there is a lot of volatility day to day, and this may serve to confuse the model. I am also not interested in short term trades. \n",
    "\n",
    "I thought the larger the time interval per data point, the more consistent to overall trends the price and volume will be. Thus I decided to include in my 'features' (the quotations will make sense towards the end of the project):  \n",
    "#####  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2-Year weekly Closing and Volume values\n",
    "\n",
    "\n",
    "\n",
    "**NOTE** Initally, I had also decided to use Institutional Holders as a Multi-Hot-Encoded feature. This information was also available through YFinance. However, a low percentage of stocks featured these values, and the set of institional holders featured over a thousand unique holders. Thus, it would not be a good feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data Extraction Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Data Extraction of Finviz, I built a StockScraper class. It utilizes BeautifulSoup,lxml, and requests to scrape information from Finviz.com\n",
    "\n",
    "StockScraper, as one of its class variables, initalizes StockScraperHelper(). Largely, this class was created to avoid overburdening the StockScraper class with methods and to differntiate any general methods that can be reused in other stock scraper classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self):\n",
    "        self.helper = StockScraperHelper()\n",
    "        self.scraped_info = []\n",
    "        self.scraped_tickers = []\n",
    "        self.HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "        #Used to prevent authorization issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main method in the class, get_all_stock_table_information goes on the inital screener page, and will iteratively call get_stock_table_information for each page available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_get_all_stock_table_information(self,url,sector='all',industry='all',country='all',market_cap='all',minPrice=2.50,maxPrice=2000,volume=200000,minimal = True):\n",
    "    \n",
    "    soup = self.get_entire_HTML_page(url)\n",
    "    total = int(soup.find('td',{'class':'count-text'},recursive=True).text.split(' ')[1])\n",
    "    iterations = total // 20\n",
    "\n",
    "    url_extension = 'r='\n",
    "    curr_tickers = 21\n",
    "\n",
    "    l = self.get_stock_table_information(soup, sector, industry, country, market_cap, minPrice, maxPrice, volume)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        next_url = url + url_extension + str(curr_tickers)\n",
    "        print(next_url)\n",
    "        next_soup = self.get_entire_HTML_page(next_url)\n",
    "        curr_tickers += 20\n",
    "        l = l + self.get_stock_table_information(next_soup, sector, industry, country, market_cap, minPrice, maxPrice, volume)\n",
    "\n",
    "    if minimal:\n",
    "        \n",
    "        keys = ('Market Cap', 'Price', 'Change', 'Volume')\n",
    "        for i in l:\n",
    "            for k in keys:\n",
    "                del i[k]\n",
    "\n",
    "    self.scraped_info = l\n",
    "    self.scraped_tickers = self.extract_tickers() `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_get_stock_table_information(self,soup,sector='all',industry='all',country='all',market_cap='all',minPrice=8,maxPrice=1000,volume=200000):\n",
    "    table_rows = soup.find_all('tr',{'class':'table-dark-row-cp'}) + soup.find_all('tr',{'class':'table-light-row-cp'}) \n",
    "    stock_list = []\n",
    "    for r in table_rows:\n",
    "        info = []\n",
    "        for child in r.descendants:\n",
    "            if child.name == 'a':\n",
    "                info.append(child.text)\n",
    "        r_dict = self.helper.row_to_dict(info)\n",
    "        stock_list.append(r_dict)\n",
    "\n",
    "    if sector != 'all':\n",
    "        stock_list = list(filter(lambda x: x['Sector'] == sector,stock_list))\n",
    "    if industry != 'all':\n",
    "        stock_list = list(filter(lambda x: x['Industry'] == industry,stock_list))\n",
    "    if country != 'all':\n",
    "        stock_list = list(filter(lambda x: x['Country'] == country,stock_list))\n",
    "    if market_cap != 'all':\n",
    "        stock_list = list(filter(lambda x: x['Market Cap']  > int(market_cap),stock_list))\n",
    "    stock_list = list(filter(lambda x: (x['Price'] > minPrice) & (x['Price'] < maxPrice),stock_list))\n",
    "    return stock_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations, Income, and Sales are scraped in their seperate method: add_RIS(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ML_P1_data_extraction, I use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_prepare_Stock_Scraper():\n",
    "    ss = StockScraper()\n",
    "    url = 'https://finviz.com/screener.ashx?v=111&'\n",
    "\n",
    "    ss.get_all_stock_table_information(url,market_cap='100000000',minPrice =2,maxPrice = 2000,volume=100000)\n",
    "    print('got table information')\n",
    "    ss.add_RIS()\n",
    "    print('added RIS')\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The total execution time with the listed parameters to build the StockScraper() was:\n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3482.4135 seconds** or roughly 58 minutes for Low Criteria Tickers\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**1588.4088 seconds** or roughly 26.5 minutes for High Criteria Tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the progress, it was time to scrape historical data with YFinance. The code for this is also found in ML_P1_data_extraction.\n",
    "To Showcase the most important method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_and_format_yfinance_data(allTickers,periodt = '1mo',intervalt = '1wk'):\n",
    "    print('period = ', periodt)\n",
    "    print('interval = ', intervalt)\n",
    "    s_tickers = ' '.join(allTickers)\n",
    "    print(s_tickers)\n",
    "    data = yf.download(tickers = allTickers, period = periodt, interval = intervalt, group_by='ticker', auto_adjust=True,threads= True)\n",
    "    rows = data.shape[0]\n",
    "    \n",
    "    queried_data = {}\n",
    "    for t in allTickers:\n",
    "        print(\"Curr ticker: \" + str(t))\n",
    "        queried_data[t] = {}\n",
    "        curr_stock_history = data[t]\n",
    "        for r in range(rows):\n",
    "            curr_date = \"Date: \" + str(curr_stock_history.index[r].date())\n",
    "            queried_data[t][str(curr_date) + ' Close'] = curr_stock_history['Close'].iloc[r]\n",
    "            queried_data[t][str(curr_date) + ' Volume'] = curr_stock_history['Volume'].iloc[r]         \n",
    "            \n",
    "        time.sleep(0.1)\n",
    "            \n",
    "    print(\"finished gathering ticker info from yfinance\")\n",
    "    return queried_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ML_P1_data_extraction file, you will find reminisce of different versions or different applications of this version of query_and_format_yfinance_data. This is due to experimentation. YFinance proved to be very unreliable in its data extraction from Yahoo! Finance. \n",
    "\n",
    "* Firstly, a lot of the methods under the YFinance library were not rigorous. They lacked error checking, and exception handling. To make this code work for all stocks, I had to modify the library quite a bit.\n",
    "\n",
    "\n",
    "* My first implementation featured additional threadding and divison to quicken the process. However, this led YFinance to skipping roughly a third of tickers, even though i introduced locking in my code!\n",
    "\n",
    "\n",
    "* My second implementation is what I ended up using (the code above). However this also featured its own problems! Specifically, it gave a lot of incorrect dates! As an example, this is a snippet demostrating how many null values were at each specified date:\n",
    "\n",
    "> Date: 2020-07-06 Close:      42\n",
    "\n",
    "> Date: 2020-07-07 Close:    1858\n",
    "\n",
    "> Date: 2020-07-13 Close:      37\n",
    "\n",
    "> Date: 2020-07-14 Close:    1858\n",
    "\n",
    "> Date: 2020-07-17 Close:    1859\n",
    "\n",
    "> Date: 2020-07-20 Close:      36\n",
    "\n",
    "> Date: 2020-07-21 Close:    1858\n",
    "\n",
    "> Date: 2020-07-27 Close:      34\n",
    "\n",
    "* As we can see, if a featured week is 2020-07-06 to 2020-07-13, then values that were between these two dates\n",
    "  are all null for all stocks (there were 1859 tickers on the 'High' Criteria list of Tickers.\n",
    "    - This bug was fixed by creating a method that pushes to a list all correct dates and filters the bad ones out for\n",
    "        both Closing and Volume values.\n",
    "        \n",
    "        \n",
    "* My third implementaiton tried to not use the yf.download() method, but rather yf.Ticker() method. While this did avoid getting the incorrect dates, it featured a lot of misisng data (Not as much as the threadding implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The total execution timet to query/scrape information with YFinance was:\n",
    " \n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**745.8681 seconds** or roughly 12.4 minutes for Low Criteria Tickers\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3895.1087 seconds** or roughly 65 minutes for High Criteria Tickers (featuring Institutional Holder querying and time.sleep of 0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation & Data Preparation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Dangerous Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I planned to only use stocks that have been in the Stock Exhange for atleast the last two years. Sadly, a lot of tickers scraped have been around for less than two years. Thus, these tickers are missing valuable information and do not meet my criteria.\n",
    "\n",
    "I however wanted to use tickers that were salvagable. As in, rather than removing all tickers that have any missing information, I created a function that removes the tickers with the most missing information and writes down the tickers that are missing a substantial amount of information in volume columns, price columns, and categorical columns.\n",
    "\n",
    "The following code did as such:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_information(filename,df,remove_worst = False):\n",
    "    pd.set_option('display.max_rows', len(df))\n",
    "    f = open(filename + '.txt','w')\n",
    "    #Divide up the df into three sections: volume per week, price per week, Non time related values\n",
    "    filter_vpw = [col for col in df if 'Volume' in col]\n",
    "    filter_ppw = [col for col in df if 'Close' in col]\n",
    "    time_rv = filter_vpw + filter_ppw\n",
    "    filter_ntrv = [col for col in df if col not in time_rv]\n",
    "\n",
    "    volume_per_week_df = df[filter_vpw]\n",
    "    price_per_week_df = df[filter_ppw]\n",
    "    categorical_values_df = df[filter_ntrv]\n",
    "    \n",
    "    serv = volume_per_week_df.isnull().sum(axis=1)\n",
    "    serv_m = serv.mean()\n",
    "    serv_std = serv.std()\n",
    "    f.write('average # null per ticker for volume columns in dataframe is: ' + str(serv.mean()) + '\\n')\n",
    "    f.write('std # null per ticker for volume columns in dataframe is: ' + str(serv_std) + '\\n')\n",
    "    serp = price_per_week_df.isnull().sum(axis=1)\n",
    "    serp_m = serp.mean()\n",
    "    serp_std = serv.std()\n",
    "    f.write('average # null per ticker for price columns in dataframe is: ' + str(serp.mean())+ '\\n')\n",
    "    f.write('std # null per ticker for price columns in dataframe is: ' + str(serp_std) + '\\n')\n",
    "    serc = categorical_values_df.isnull().sum(axis=1)\n",
    "    serc_m = serc.mean()\n",
    "    serc_std = serc.std()\n",
    "    f.write('average # null per ticker for categorical columns in dataframe is: ' + str(serc.mean())+ '\\n')\n",
    "    f.write('std #null per ticker for categorical columns in dataframe is: ' + str(serc_std) + '\\n')\n",
    "    \n",
    "    #get subset where the amount of nulls is greater than one standard deviation away from the mean, then sort. \n",
    "    #This'll give me an ide aof which stocks to drop.\n",
    "    serv_outliers = serv[(serv >= serv_m + 1*serv_std)]\n",
    "    serv_outliers.sort_values(ascending=False,inplace=True,na_position='first')\n",
    "    \n",
    "    serp_outliers = serp[(serp >= serp_m + 1*serp_std)]\n",
    "    serp_outliers.sort_values(ascending=False,inplace=True,na_position='first')\n",
    "    \n",
    "    serc_outliers = serc[(serc >= serc_m + 1*serc_std)]\n",
    "    serc_outliers.sort_values(ascending=False,inplace=True,na_position='first')\n",
    "\n",
    "    size = str(len(df.index))\n",
    "    \n",
    "    f.write(\"VOLUME NULL OUTLIERS: \\n\")\n",
    "    f.write(str(serv_outliers.shape[0]) + \"/\" + size + \" are one standard deviation away from average null values per ticker\\n\")\n",
    "    f.write(serv_outliers.to_string())\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "    f.write(\"PRICE NULL OUTLIERS: \\n\")\n",
    "    f.write(str(serp_outliers.shape[0]) +\"/\" + size + \" are one standard deviation away from average null values per ticker\\n\")\n",
    "    f.write(serp_outliers.to_string())\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "    f.write(\"CATEGORICAL NULL OUTLIERS: \\n\")\n",
    "    f.write(str(serc_outliers.shape[0]) + \"/\" + size + \" are one standard deviation away from average null values per ticker\\n\")\n",
    "    f.write(serc_outliers.to_string())\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "    \n",
    "    \n",
    "    .\n",
    "    serv_i = serv_outliers.index\n",
    "    serp_i = serp_outliers.index\n",
    "    serc_i = serc_outliers.index\n",
    "    f.write(\"BAD STOCKS\")\n",
    "    bad_stocks = set(serv_i).intersection(serp_i).intersection(serc_i)\n",
    "    for b in bad_stocks:\n",
    "        f.write(b)\n",
    "        f.write('\\n\\n')\n",
    "    \n",
    "    if remove_worst == True:\n",
    "        for i in serp_i:\n",
    "            df.drop(i,inplace=True)\n",
    "        return df\n",
    "    pd.reset_option('display.max_rows')\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively applying this function demostrated a few things:\n",
    "* If a 'Close' column was missing a value at a certain date, then the volume was also missing at the same date.\n",
    "* If a Stock was missing a value at a certain certain date, then the dates were almost entirely before the debut of the Ticker in the Stock Exchange\n",
    "\n",
    "Thus I learned that in my case, I would get practically the same result if I simply created a function that dropped rows if the start date Volume and Price columns were empty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Google Machine Learning Crash Course states that if a feature value appears less than 5 times, then it is not a good feature. I grouped all the tickers in the DataFrame into Sectors and Industry and counted occurences of each.\n",
    "\n",
    "` df_groupby_sector = df.groupby('Sector').count() `\n",
    "\n",
    "` df_groupby_industry = df.groupby('Industry').count() ` \n",
    "    \n",
    "Each sector had plenty of examples, thus I did not have to cut any sectors and their associated tickers.\n",
    "However, there were industries with less than 5 examples. I decided to cut those industries and their associated tickers with the following code: \n",
    "\n",
    "\n",
    "For countries, the majority of tickers belonged in the United States. However, there were tickers that belonged to other countries. Some of them, like Italy and Ukraine had less than 5 occurences. Thus I removed any country alongside their tickers that had less than 5 occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_remove_bad_ticks_by_group(df,occurences,group_n):\n",
    "    g = df.groupby(df[group_n]).count()\n",
    "    \n",
    "    bad_g = list(g[g['Sector'] < occurences].index.values)\n",
    "    print(bad_g)\n",
    "    df_i = df[[group_n]]\n",
    "    for index,row in df_i.iterrows():\n",
    "        if row.values[0] in bad_g:\n",
    "            df.drop(index,inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most features that were scraped from Finviz are features that will be one-hot encoded for the model. These features include:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Sector, Industry, Country, Recommendations**\n",
    "\n",
    "Every stock has a sector, industry, and country value. The recommendations feature has null values. Thus for recommendation I will be adding another OHE feature that tracks 'Has Recommendation Value'\n",
    "\n",
    "All this is handled by the same function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_one_hot_encode(df,c_name,contains_null = False,null_value = None,sparsev=False):\n",
    "    r_df = pd.concat([df,pd.get_dummies(df[c_name],prefix=c_name,sparse=sparsev)],axis=1,sort=False)\n",
    "    \n",
    "    def check_for_null(x):\n",
    "        \n",
    "        if null_value is None:\n",
    "            if pd.isnull(x):\n",
    "                print_global_count() # To see if I miss a null or not.\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        else:\n",
    "            if x is null_value:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "            \n",
    "            \n",
    "    if contains_null == True:\n",
    "        r_df['Contain ' + c_name] = df[c_name].apply(check_for_null)\n",
    "    \n",
    "    r_df.drop([c_name],axis=1,inplace=True)\n",
    "    print('Done one hot encoding: ' , c_name)\n",
    "\n",
    "    reset_global_counter()\n",
    "    return r_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While preparing categorical features was a simple task, preparing non-categorical features proved to be slightly trickier.\n",
    "\n",
    "First I had to deal with missing 'Income' and 'Sales' values. On average, there were substantially more missing 'Income' values than 'Sales' values. I used this fact in my development of inferred 'Income' and 'Sales' values.\n",
    "\n",
    "First I dealt with missing Income values. \n",
    "\n",
    "* A dataframe with tickers with a null income value was initalized.\n",
    "* If for the ticker the sales value was not null:\n",
    "    * Get group of tickers with the same sector and get median value of the ratio of the sector's income and sales values.\n",
    "        * `median_income_div_sales = (df_sect['Income'] / df_sect['Sales']).median()`\n",
    "    * Multiply the sales value with median_income_div_sales\n",
    "* If for the ticker the sales value was null:\n",
    "    * Calculate and place mean income value for the tickers in the same indutsry \n",
    "        * ` m_val_by_industry = in_df_indust.get_group(industry).mean()`\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "Similar logic was used to fill the missing 'Sales' values, but now we knew for sure that every ticker has a 'Income' value thus we can specialize the sales inferred value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My DataFrame is set up so each row represents a ticker where the ticker name is the index, and the columns include all the features.\n",
    "\n",
    "There were features that had to be scaled row wise, and features that had to be scaled column wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row-wise Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date 'Volume' and 'Closing' values had to be scaled row wise. \n",
    "I was on the fence if stock closing values are heteroscedastic. A paper by [JL Sharma](https://www.tandfonline.com/doi/abs/10.1080/096031096334132) provides proof that stock returns are heteroscedastic but no evidence online has to do with prices and volume.\n",
    "\n",
    "Upon reaserch online, many articles demostrated [price](https://www.investopedia.com/articles/investing/102014/lognormal-and-normal-distribution.asp) values of stocks are lognormal\n",
    "\n",
    "Simply put, \n",
    "\n",
    "If the random variable X is log-normally distributed, then Y = ln(X) has a normal distribution.\n",
    "\n",
    "\n",
    "Thus for the closing values, I will be first transforming the values to ln(X) then I will do Standard scale.\n",
    "\n",
    "For volume I initially thought of using MinMax scaling however of fear (probably unreasonable, and this is where my inexperienced of data analysis and ML shines) of using two different scaling mechanisms for likely correlated values is not the best idea. Thus I decided to scale volume the same as I scaled closing values. \n",
    "\n",
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{new} =  \\frac{x-\\mu}{\\sigma}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code used for both Volume and Price is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_log_normal_scale_rows(df):\n",
    "    \n",
    "    for column in df:\n",
    "        df.loc[:,column] = np.log(df[column])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df2 = pd.DataFrame(columns = df.columns,index=df.index,data=scaler.fit_transform(df.values.T).T)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column-wise Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Income' and 'Sales' values had to be scaled column wise. Intutiively I thought using a MinMax Scalar per sector would be best rather than using a single MinMax Scalar for the entire columns of 'Income' and 'Sales'. This very well may have been an error from my part but I believed ticker's incomes and sales should be compared to their sector's incomes and sales. Dividing it up by Industry would stretch the groups too thin and would ruin this feature for the model\n",
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  x_{new} = \\frac{x-x_{min}}{x_{max}-x_{min}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code used for both Income and Sales is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_scale_MinMax_by_Sector(df,s_or_i='Sales'):\n",
    "    g = df.groupby(df['Sector'])\n",
    "    df_section = df[[s_or_i]]\n",
    "    names = g.groups.keys()\n",
    "    scaler = MinMaxScaler()\n",
    "    for n in names:\n",
    "\n",
    "        dfn = g.get_group(n)[[s_or_i]]\n",
    "\n",
    "        dfn_scaled = pd.DataFrame(index=dfn.index,data=scaler.fit_transform(dfn.values))\n",
    "        \n",
    "        df_section[df_section.index.isin(list(dfn.index))] = dfn_scaled\n",
    "\n",
    "        dfn_scaled.to_excel('minmax'+n+'.xlsx')\n",
    "    return df_section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Different Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the current code, I have generated a scaled and one hot encoded Dataframe with features being the columns of the DataFrame and the rows being the examples. \n",
    "\n",
    "The columns (features) consist of : \n",
    "* Sector, Industry, Recommendations and Country One-Hot Encoded\n",
    "* Sales and Income scaled by MinMax\n",
    "* Volume and Date scaled lognormally for involved Dates\n",
    "\n",
    "I have created two datasets:\n",
    "* **2YStockDFHighCriteria** - Features less examples but stocks strating with a higher Market Capital\n",
    "* **2YStockDFLowCriteria** - Features more examples but stocks starting with a lower Market Capital\n",
    "\n",
    "\n",
    "Depending on the needs of the models I will be developing, I wrote a couple of methods to create different forms of these dataset. Initally I will be experimenting with the low criteria dataset and these names are in regards to that dataset. \n",
    "1. Dataset with Sector removed. (**2YStockDFSR**)\n",
    "2. Dataset with Industry removed. (**2YStockDFIR**)\n",
    "3. Dataset with Volume columns removed. (**2YStockDFVR**)\n",
    "4. Dataset with a new feature, Price/Volume, lognormally scaled which replaces the Price and Volume columns (**2YStockDFRatio**)\n",
    "5. Dataset where the time frame is reduced to 6 months resulting in 4 times more examples and ~4 times less Dated features\n",
    "    * This will be applied to the original DataFrame as well as the Dataframe of part 3. (**2YStockDFSplit** , **2YStockDFRatioSplit**)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms to be Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The main purpose of this project is to familiarize myself with the world of Machine learning, and to have a little fun along the way. For me, fun means experiementing and comparing results from different algorithms on different data sets, alongside trying new ideas and see if they lead to an improvement.\n",
    "\n",
    "In this project I will be implementing three different Machine Learning Algorithms:\n",
    "* Long Short Term Memory (LSTM) Recurrent Neural Network\n",
    "* Support Vector Machine (SVM)\n",
    "* Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Term Short Memory Brief Summary\n",
    "\n",
    "Lets say we have a phrase:\n",
    "> \"I am a veterinarian. I perform ___________ on ________________.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If asked what are the words that are missing, one would most likely say 'surgey' and 'animal'. But how did we reach this conclusion?\n",
    "\n",
    "Well, we looked at the words that came before! We kept them in memory and we filled in the blank knowing what came before in the phrase. If we never kept the previous words in memory, we would never be able to guess what the blank would be! And after filling the first blank, we didn't regard the second blank as independent. We followed through and knew that the first blank had some correlation with the second blank.\n",
    "\n",
    "Most Machine Learning algorithms cannot do this. This is because in conventional algorithms, all test cases are considered independent. Reccurent Neural Networks fix this problem by introducing loops in the NN. These loops can be thought of as a chain of copies of the same NN, each passing a message to the next (few) NN on the chain.\n",
    "\n",
    "Long Short Term Memory Neural Networks are a special kind of Recurrent Neural Network that allows for storing important past information and forgetting past information that is not important.\n",
    "\n",
    "The main mechanism of LSTMs is the cell state, a part of a 'neuron' that realizes the 'memory' aspect of the NN. It acts like a conveyor belt from the previous NN repeating module in the sequence, to the current, to the next NN repeating module in the sequence. The cell state is updated by three gates:\n",
    "\n",
    "* **Forget Gate** : Comprised of a layer (sigmoid) which outputs a value from 0 to 1 (0 being get rid of the information, 1 means keep all of it) deciding on how much and what information should be forgotten. Note that the information is not dropped here, just the forget values are given. \n",
    "\n",
    "* **Input Gate** : Comprised of 2 layers (sigmoid and tanh) pushing how much and what information gets stored in the cell with the updated forget values from the forget gate.. \n",
    "\n",
    "* **Output Gate** : Comprised of 2 layers (sigmoid and tanh) determining how much of the current cell information gets pushed to the next.\n",
    "\n",
    "For a more in depth explanation of Long Short Term Memory, please check out the following sources:\n",
    "\n",
    "[Source 1](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) [Source 2](https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/) [Source 3](https://www.asimovinstitute.org/neural-network-zoo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/LTSMimg.png\"  width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Brief Summary\n",
    "\n",
    "We can think of features of examples in a training set representing different dimensions. So if there are N features, there are N dimensions. Each example has their own configuration of values in each of these dimensions. Support Vector Machines are a supervised classificiation model which divides this N dimensional space with a N-1 dimensional Hyperplane.\n",
    "\n",
    "As an example, let's say we have two features, x and y, which are the dimensions in the space S. We can think of these features as the axis x and y of the common 2D Cartesian Coordinate System. Each example is its own point in S:\n",
    "\n",
    "* [ (2,5) , (13, -5) ,  ( -4 , -4) ]\n",
    "\n",
    "Let's say there are two versions of the points: Green points and Red points as shown in my beautiful work in 'Paint':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/SVMv1.png\"  width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S is a 2D space thus the hyperplane that will be used is a line (1D). The goal of Support Vector Machines is where to place this hyperplane, allowing few misclassfications, so that the margin between the Green and Red points from the hyperplane is maximized. This margin is called the Soft Margin, and the points that are 'most important' in the decision on where to place and how to orient this hyperplane (and the points where the soft margin is calculated) are the support vectors.\n",
    "\n",
    "Good placement of the hyperplane increases the chance of new data being classified correctly.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/SVMv2.png\"  width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of the time, points are not linerally seperable. A precise placement and orientation of a hyperplane is not possible by simply finding good support vectors and drawing the hyerplane. Thus Support Vector Machines have kernels which functinoally transforms/compares the training data into higher dimensions to make them linearaly seperable.\n",
    "\n",
    "Two popular kernels are:\n",
    "* Polynomial Kernels : creates new variables which are polynomial transformations of old ones to fit a hyperplane between them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/polykern.png\"  width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Radial Basis Function Kernels : Behaves as a weighted nearest neighbor model which gives greater weight to points closer to the new observed point to help classify the new point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rbk.png\"  width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For a more in depth explanation of Support Vector Machines, please check out the following sources:\n",
    "\n",
    "\n",
    "[Source 1](https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html)  [Source 2](https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496) [Source 3](https://www.youtube.com/watch?v=efR1C6CvhmE&vl=en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Brief Summary\n",
    "\n",
    "Logistic Regression is very closely related to Multiple Regression. A quick overview of Multiple Regression will give us better insight on Logistic Regression\n",
    "\n",
    "#### Multiple Regression Quick overview\n",
    "\n",
    "Suppose we have features such as humidity, temperature,windspeed, longitude, and latitude ( $ x_{1} \\rightarrow x_{5} $ respectively ) and want to predict how many inches of rainfall will occur later on the day  $ y_{label} $\n",
    "\n",
    "Each example in our dataset thus corresponds to a point ( $ x_{1} , x_{2} , x_{3} , x_{4} , x_{5} , y_{label} $ ) on a 6-dimensional space.\n",
    "\n",
    "To predict the label, we can opt to fit a hyperplane through this space.\n",
    "\n",
    "Then to fit a hyperplane to best predict the label, we use a plane equation:\n",
    "\n",
    "$$ y^{'} = b \\sum_{i = 1 }^{i \\in F} w_{i}x_{i} \\\\ $$\n",
    "\n",
    "where:\n",
    "* $ y^{'} $ is the predicted inches of rainfall\n",
    "* $ b $ is the bias (y-intercept) \n",
    "* $ x_{i} $ is feature i \n",
    "* $ w_{i} $ is the weight (slope) which basically defines how much of a feature should be used to calculate y\n",
    "* $ F $ is the amount of features per example.\n",
    "\n",
    "Initally, multiple regression fits a random hyperplane. However using loss measurements such as (but not limited to)\n",
    "\n",
    "**Least Squared Errors ( $ L_{2} $ )** ,\n",
    "\n",
    "\n",
    " $$ L_{2} = (y_{label} - prediction( x_{1} , x_{2} , ..... , x_{n} ) )^2 $$\n",
    " \n",
    " and **Mean Squared Error (MSE)** \n",
    " \n",
    " $$ MSE = \\frac{1}{N} \\sum_{ (x_{1} , ..... , x_{n} , y_{label}) \\in D} L_{2}\\\\ $$\n",
    " \n",
    " where:\n",
    " * $ N $ is the size of examples\n",
    " * $ D $ is the example set\n",
    " \n",
    " alongside **Gradient Descent** (modify weights based on negative gradient  $  - \\nabla f = - \\left(\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},...,\\frac{\\partial f}{\\partial x_n}\\right)$ of weight to loss to locate a local minimum) , **learning rate** (scalar which the gradient is multiplied with to decide the size of the steps toward the local minimum), and **epochs** (how many times our entire dataset is passed through the algorithm) to reach a best hyperplane for the training set. \n",
    " \n",
    "We can also use techniques such as feature crossing (i.e multiplied features together) and bucketing (tuning continous variables into categorical values) creating new features to help the model best fit the data.\n",
    "\n",
    "#### Back to Logistic Regression\n",
    "\n",
    "Rather than predicting a continuous label, logistic regression is a binary classification supervised model which outputs the __probability__ of a given example being _True_ on a scale from 0 to 1. This is done by using the sigmoid function :\n",
    "\n",
    "$$ Sigmoid(z) = y^{'} = \\frac{1}{1+e^{-(z)}} $$\n",
    "\n",
    "where z is known as the log-odds function:\n",
    "\n",
    "$$ z = log(\\frac{y}{1-y}) $$\n",
    "\n",
    "where y is the output of the plane equation mentioned above.\n",
    "\n",
    "While the loss function for linear regression is squared loss, the loss function for logistic regression is **Log Loss**:\n",
    "\n",
    "$$ Log Loss = \\sum_{(x_{1} , ..... , x_{n} , y_{label})} -ylog(y^{'})-(1-y)log(1-y^{'}) $$\n",
    "\n",
    "The asymptotic nature of logistic regression drives loss toward 0 in high dimensions. Thus regularization (penalizing complexity of a model) is necassary for logsitic regression. \n",
    "\n",
    "Logitic regression models use strategies such as $ L_{1}$, $L_{2}$, and **Early Stopping** to dampen the complexity.\n",
    " \n",
    " For a more in depth explanation of Logistic Regression, please check out the following sources:\n",
    "\n",
    "\n",
    "[Google MLCC](https://developers.google.com/machine-learning/crash-course) [Source 1](https://christophm.github.io/interpretable-ml-book/logistic.html)  [Source 2](https://www.kdnuggets.com/2020/03/linear-logistic-regression-explained.html) [Source 3](https://www.youtube.com/watch?v=OCwZyYH14uw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM require a unique shape due to their time-series nature.\n",
    "Data should be inputted to a LSTM model in a [Samples, Time Steps, Features] format, where:\n",
    "* Samples: The distinct samples of the dataset\n",
    "* Time Steps: The Time-Series data of the sample\n",
    "* Feature: The features present at each time step.\n",
    "\n",
    "The DataFrames created through the preperation phase do not have such a format. Thus I used the following method to prepare the data in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_LSTM_data(df):\n",
    "    vpw, ppw, cv = p.filtered_columns(df) \n",
    "    \n",
    "    time_related_cols = [col for col in df if col not in cv]\n",
    "    \n",
    "    time_related_df = df[time_related_cols]\n",
    "    \n",
    "    time_series_df_by_ticker = []\n",
    "    ticker_names = []\n",
    "    y_values_tsdbt = []\n",
    "    \n",
    "    col_names = time_related_df.columns\n",
    "    col_names_length = len(col_names)\n",
    "    g = 0\n",
    "    '''first, we make a list of dataframes where each dataframe is a ticker\n",
    "        where the rows are the Dates and the columns are price and volume at \n",
    "        that date. \n",
    "    '''\n",
    "    for i, r in time_related_df.iterrows():\n",
    "        print(g)\n",
    "        g+=1\n",
    "        \n",
    "        j = 0 \n",
    "        tdf = pd.DataFrame(columns=['Price', 'Volume'])\n",
    "        ticker_names.append(r.name)\n",
    "        \n",
    "        while j < col_names_length-1:   \n",
    "            close = r[col_names[j]]\n",
    "            volume = r[col_names[j+1]]\n",
    "            date = col_names[j][:16]\n",
    "            tdf.loc[date] = [close, volume]\n",
    "            j+=2\n",
    "            \n",
    "        y_row = tdf.iloc[-1]\n",
    "        tdf = tdf[:-1]\n",
    "        \n",
    "        time_series_df_by_ticker.append(tdf)\n",
    "        y_values_tsdbt.append(y_row)\n",
    "        \n",
    "        #print('close', close , ' volume' , volume, ' date' , date)\n",
    "    \n",
    "    \n",
    "    e.save_obj(time_series_df_by_ticker,'x_ticker_time_series_low_criteria')\n",
    "    e.save_obj(y_values_tsdbt,'y_ticker_time_series_low_criteria')\n",
    "    return ticker_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create the actual model.\n",
    "\n",
    "#### Regularization Technique\n",
    "To create the model I will be using Dropout as the main regularization technique.\n",
    "\n",
    "\n",
    "Dropout in every iteration goes through the layers of a network and sets a probability of removing a node at each layer.\n",
    "This will cause all the ingoing an doutgoing links from the removed node to also be removed and lead to a diminished network, forcing the network to never compeltely rely on any one node as it can be removed at any time.\n",
    "\n",
    "This'll also prevent scenarioswhere layers co-adapt to correct mistakes from prior layers. Dropout regularization works in synch with backwards propogation.\n",
    "\n",
    "I have essentially zero knowledge in the layer architecture thus I had to experiment (initally randomly) on the layer architecture until I reached a satisfactory composition (satisfactory relating to the results and MSE)\n",
    "\n",
    "The composition I will be displaying in the notebook is one with 3 LSTM layers, each with 64 units. After each LSTM layer there is a dropout layer for the regularization.\n",
    "\n",
    "The code for this composition is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_model(learning_rate,x_train):\n",
    "    model = Sequential()\n",
    "    \n",
    "    df = pd.read_pickle('../data/2YStockDFLowCriteria.pkl')\n",
    "    \n",
    "    \n",
    "    #layer 1\n",
    "    model.add(LSTM(units=64,return_sequences=True,input_shape=x_train.shape[1:]))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    #layer 2\n",
    "    model.add(LSTM(units=64,return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    #layer 3\n",
    "    model.add(LSTM(units=64,return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    \n",
    "    #model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I train the model with the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM_model(model, x_vals, y_vals,batchsize, epochs,v_split=0.2):\n",
    "    \n",
    "    history = model.fit(x=x_vals, y=y_vals, batch_size=batchsize,\n",
    "                      epochs=epochs, shuffle=False,validation_split=v_split) \n",
    "\n",
    "\n",
    "    # The list of epochs is stored separately from the rest of history.\n",
    "    epochs = history.epoch\n",
    "\n",
    "    # To track the progression of training, gather a snapshot\n",
    "    # of the model's mean squared error at each epoch. \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    mse = hist[\"mean_squared_error\"]\n",
    "\n",
    "    return epochs, mse, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, after training the model we will need to test it on data it has never seen before to really judge its effectiveness.\n",
    "\n",
    "I did such with the following code. Sadly however, I had not saved the scaler used to scale the prices and volumes. Thus I had to recreate the scaler in the following code so I could compare the predicted and actual stock prices, not prices that are standardized and a logarithmic transformation of the original. \n",
    "\n",
    "I also faced issues with broadcasting shapes of the dataframes I was working with. I was unable to figure out the real solution but I was able to work around the issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_LSTM_results(model,x_test,y_test,ticker_names):\n",
    "    df = pd.read_pickle('../data/Low_Criteria_Pre_price_v_scale.pkl')\n",
    "    #df.to_excel('real_price.xlsx')\n",
    "    vpw,ppw,cv = p.filtered_columns(df)\n",
    "    p.print_full(ppw.head())\n",
    "    \n",
    "    df = ppw\n",
    "    \n",
    "    for column in df:\n",
    "        df.loc[:,column] = np.log(df[column])\n",
    "    \n",
    "\n",
    "    '''Because of broadcast shape issues i had to do this part slightly spaghetti'''\n",
    "    \n",
    "    \n",
    "    #first I make new dataframe with only test values. Note log is already calculated.\n",
    "    df4 = df.loc[ticker_names]\n",
    "    \n",
    "    #then i split the semi and last column:\n",
    "    semi_last_col = copy.deepcopy(df4.iloc[:,-2])\n",
    "    last_col = copy.deepcopy(df4.iloc[:,-1])\n",
    "    \n",
    "    #then I run the predict.\n",
    "    x_test = numpify_LSTM_data(x_test)\n",
    "    y_test = numpify_LSTM_data(y_test)\n",
    "\n",
    "    predicted_stock_prices = model.predict(x_test,batch_size = 1)\n",
    "    \n",
    "    scaler2 = StandardScaler()\n",
    "    \n",
    "    #then i standard scale\n",
    "    df4 = pd.DataFrame(columns = df4.columns,index=df4.index,data=scaler2.fit_transform(df4.values.T).T)\n",
    "    \n",
    "    #then i place the predicted price as last column\n",
    "    df4.iloc[:,-1] = predicted_stock_prices\n",
    "    \n",
    "    #then i inverse transform.\n",
    "    df4 = pd.DataFrame(columns=df4.columns,index=df4.index,data=scaler2.inverse_transform(df4.values.T).T)\n",
    "    for column in df4:\n",
    "        df4.loc[:,column] = np.exp(df4[column])\n",
    "    \n",
    "    \n",
    "    compare_df = pd.concat([np.exp(semi_last_col),np.exp(last_col),df4.iloc[:,-1]],axis=1,sort=False)\n",
    "    compare_df.columns = ['Previous Price', 'Real Next Price','Predicted Next Price']\n",
    "    compare_df['Error %'] = compare_df.apply(lambda row: (np.abs((row.iloc[1]-row.iloc[2]))/np.abs((row.iloc[1]+row.iloc[2])/2))*100, axis=1)\n",
    "    compare_df['Real % Change'] = compare_df.apply(lambda row: ((row.iloc[1] - row.iloc[0])/np.abs(row.iloc[0]))*100,axis=1)\n",
    "    compare_df['Predicted % Change'] = compare_df.apply(lambda row: ((row.iloc[2] - row.iloc[0])/np.abs(row.iloc[0]))*100,axis=1)\n",
    "    compare_df['Difference in % Change'] = compare_df.apply(lambda row: (row.loc['Real % Change'] - row.loc['Predicted % Change']),axis=1)\n",
    "    # to see if model predicted atleast 3 % increase when there was a 3% increase.\n",
    "    compare_df['Real Increase 3%'] = compare_df.apply(lambda row: (True if (row.loc['Real % Change'] >= 3.00) else False),axis=1)\n",
    "    compare_df['Predicted Increase 3%'] = compare_df.apply(lambda row: (True if  (row.loc['Predicted % Change'] >= 3.00) else False),axis=1)\n",
    "    \n",
    "    #Could've done this as a if real and predicted increase 3% then True else false but I had implemented this before those two.\n",
    "    compare_df['Right Investment Prediction'] = compare_df.apply(lambda row: (True if ((row.loc['Real % Change'] >= 3.00) and (row.loc['Predicted % Change'] >= 3.00)) else False),axis=1)\n",
    "    \n",
    "    \n",
    "    compare_df.to_excel('LSTM_SPP_results2.xlsx')\n",
    "    \n",
    "    #Some statistics:\n",
    "    true_count_p = compare_df['Predicted Increase 3%'].sum()\n",
    "    true_count_r = compare_df['Real Increase 3%'].sum()\n",
    "    true_count_t = compare_df['Right Investment Prediction'].sum()\n",
    "    mean_error_pred = compare_df['Error %'].mean()\n",
    "    mean_perc_change = compare_df['Real % Change'].mean()\n",
    "    mean_p_perc_change = compare_df['Predicted % Change'].mean()\n",
    "    mean_diff_perc = compare_df['Difference in % Change'].mean()\n",
    "    \n",
    "    print('True count predicted 3% increase: ',true_count_p)\n",
    "    print('True count real 3% increase: ', true_count_r)\n",
    "    print('True count predicted and real 3% increase', true_count_t)\n",
    "    print('Mean error percentage between real and predicted',mean_error_pred)\n",
    "    print('Mean real % change: ',mean_perc_change)\n",
    "    print('Mean predicted % change: ', mean_p_perc_change)\n",
    "    print('Mean difference in % change: ',mean_diff_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The driver code to automate the entire LSTM process is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_process():\n",
    "     #df = pd.read_pickle('../data/2YStockDFLowCriteria.pkl')\n",
    "    #ticker_names = prepare_LSTM_data(df)\n",
    "    #e.save_obj(ticker_names,'ticker_names')\n",
    "    \n",
    "    LSTM_data_X = e.load_obj('../data/x_ticker_time_series_low_criteria')\n",
    "    LSTM_data_Y = fix_y_values(e.load_obj('../data/y_ticker_time_series_low_criteria'))\n",
    "    ticker_names = e.load_obj('ticker_names')\n",
    "    \n",
    "    x_train = LSTM_data_X[0:3041]\n",
    "    x_validate = LSTM_data_X[2366:3041]\n",
    "    x_test = LSTM_data_X[3041:]\n",
    "    \n",
    "    y_train = LSTM_data_Y[0:3041]\n",
    "    y_validate = LSTM_data_Y[2366:3041]\n",
    "    y_test = LSTM_data_Y[3041:]\n",
    "    \n",
    "    ticker_names = ticker_names[3041:]\n",
    "    \n",
    "    x_train = numpify_LSTM_data(x_train)\n",
    "    y_train = numpify_LSTM_data(y_train)\n",
    "    print('ready to create model')\n",
    "    model = create_LSTM_model(0.005,x_train)\n",
    "    print('ready to train model')\n",
    "    epochs,mse,history = train_LSTM_model(model, x_train, y_train, 64, 20,v_split=0.2)\n",
    "    print('model trained.')\n",
    "    list_of_metrics_to_plot = ['accuracy'] \n",
    "    plot_the_loss_curve(epochs, mse)\n",
    "\n",
    "    compare_LSTM_results(model,x_test,y_test,ticker_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Results:\n",
    "The MSE loss curve with the configuration featured above was as follows:\n",
    "<img src=\"img/LossCurve.png\"  width=\"600\"/>\n",
    "\n",
    "It seems as I could've trained less epochs with the featured batch size with roughly the same outcome.\n",
    "\n",
    "The **MSE** before inverse_transforming the data was: **0.0962**\n",
    "\n",
    "###### Some statistics with the featured configuration\n",
    "True count predicted 3% increase:  **89**\n",
    "\n",
    "True count real 3% increase:  **95**\n",
    "\n",
    "True count predicted and real 3% increase **43**\n",
    "\n",
    "Mean error percentage between real and predicted **4.447351038441611**\n",
    "\n",
    "Mean real % change:  **1.030980360887372**\n",
    "\n",
    "Mean predicted % change:  **0.17531416823364612**\n",
    "\n",
    "Mean difference in % change:  **0.8556661926537258**\n",
    "\n",
    "\n",
    "The stock-by-stock predictions with the featured configurations can be found on the excel sheet 'LSTM_SPP_results2.xlsx'\n",
    "\n",
    "### Important Points:\n",
    "* Overall, one shouldn't be trust such a simple LSTM with their money\n",
    "\n",
    "\n",
    "* The LSTM was bad at predicting large changes in stock prices. This is likely due to such drastic changes in stock prices not being completely correlated with simply price and volume data through time\n",
    "    * As proof: The median error rate of the above configuration was %2.79 compared to the mean error rate of %4.45. This disparity is likely due to some stocks featuring drastic changes in their pricing and the LSTM not being able to keep up!   \n",
    "        * This becomes even more apparent when you notice the disparity between the real mean % change and the predicted mean % change.\n",
    "\n",
    "\n",
    "* The LSTM performed much better with lower batch sizes. With a similar epoch, lower learning rate, and a batch size of 1, the model was able to bring the mean error rate down to mid-high 3%! The median error rate was also down to low 2%! As each stock is its own business and likely not very correlated with a random other stock, an online batch size approach seems to be optimal\n",
    "\n",
    "\n",
    "* The results published on this Jupyter are not the best this built of a LSTM can do. However, the error rates are still too high to bet on this LSTM to make you any money.\n",
    "    * To improve on this LSTM we can also add new time-series features, such as Turnover, RSI, short ratio, etc.\n",
    "\n",
    "\n",
    "* If you would like to discuss other configurations, and what I noticed to be the optimal configuration, reach out at eneadodi@umass.edu!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM results were initally thought to be used as a feature to the logistic regression model. However, it isn't bad to see how it would perform on its own!\n",
    "\n",
    "I wanted to create as many examples as I could for the SVM. Thus I divided up the two year data of each stock into 5 equal piece of 22 weeks. The last week was used as the label (went up 3% or not) thus the SVM would be working with 21 weeks of price and volume at a time. This division of the data resulted in four times more data with half the features! Later I also removed the majority of the features, such as the Sector, Industry, and Country One Hot Encoded features, resulting in only 48 features per example and over 13600 examples! **Way** better than the original 3380 examples and 386 features\n",
    "\n",
    "Finally, I decided to not use the lognormally scaled Price and Volume features. Instead, I used a new feature:\n",
    "\n",
    "$$ \\frac{Price}{Volume} $$\n",
    "\n",
    "This led to the amount of features onces again decreasing to now only 27!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, while having the data and in many forms, the SVM required it's own data preperation. I prepared the data with the two following methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_process1():\n",
    "    \n",
    "    df = pd.read_pickle('../data/Low_Criteria_Pre_price_v_scale.pkl')\n",
    "    dstart2y = [2018,8,6]\n",
    "    dend = [2020,8,10]\n",
    "\n",
    "    final_two_split, df2 = split_dates_v_and_p(df,dstart2y,dend,5)\n",
    "    \n",
    "    df2 = df2.iloc[:13580]\n",
    "    \n",
    "    #now we split price and volume separately and remove last two columns of each split df\n",
    "\n",
    "    vpw,ppw,cw = p.filtered_columns(df2)\n",
    "    \n",
    "    label_price = copy.deepcopy(ppw[ppw.columns[-2:]])\n",
    "    ppw = ppw[ppw.columns[:-1]]\n",
    "    vpw = vpw[vpw.columns[:-1]]\n",
    "\n",
    "    #now we combine the columns again.\n",
    "    df2 = pd.concat([cw,vpw,ppw],axis=1)\n",
    "\n",
    "\n",
    "    #we make the label column and join it to df2.\n",
    "    label_price['Gain %'] =  label_price.apply(lambda row: ((row.iloc[1] - row.iloc[0])/np.abs(row.iloc[0]))*100,axis=1)\n",
    "    label_price['label'] = label_price.apply(lambda row: (1 if (row.loc['Gain %'] >= 3) else 0),axis=1)\n",
    "\n",
    "    l_col = label_price['label']\n",
    "    print(l_col.shape)\n",
    "    print(df2.shape)\n",
    "    df2 = pd.concat([df2,l_col],axis =1)\n",
    "    e.save_obj(df2,'second_check_pointSVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_process2():\n",
    "    df = pd.read_pickle('second_check_pointSVM.pkl')\n",
    "    #df.to_excel('sffs.xlsx')\n",
    "    df = p.price_volume_ratio_feature_creator(df,date_version=False)\n",
    "    label = df['label']\n",
    "    df.drop(columns='label',inplace=True)\n",
    "    df['Label'] = label\n",
    "    print('made it here!')\n",
    "    e.save_obj(df,'third_check_pointSVM')\n",
    "    print('done with svm process 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creation of a SVM was much simpler than a LSTM. The entire data preparation, model creation, model training, and model testing process was automated by a single method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_process():\n",
    "    SVM_process1()\n",
    "    SVM_process2()\n",
    "    df = pd.read_pickle('../data/third_check_pointSVM.pkl')\n",
    "    \n",
    "    df_y = df['Label']\n",
    "    #on the SVM, we will not be using the Sector,Industry, Country\n",
    "    filt = []\n",
    "    filt.extend([c for c in df.columns if 'Industry_' in c])\n",
    "    filt.extend([c for c in df.columns if 'Sector_' in c])\n",
    "    filt.extend([c for c in df.columns if 'Country_' in c])\n",
    "    filt.append('Label')\n",
    "    filt = list(set(filt)) #to make sure there's no repeated values.\n",
    "    print('length of filter columns is: ' , len(filt))\n",
    "    print('pre')\n",
    "    print(df.shape)\n",
    "    df_x = df.drop(filt,axis=1)\n",
    "    \n",
    "    x_data = df_x.to_numpy()\n",
    "    y_data = df_y.to_numpy()\n",
    "    print('post')\n",
    "    print(x_data.shape)\n",
    "    \n",
    "    '''train/validate with 75% of the data. This was chosen \n",
    "    to avoid training on the last 22 weeks because the SVM \n",
    "    will in the future work in conjunction with logistic regression\n",
    "    and will be looking at the last 22 weeks there.\n",
    "    '''\n",
    "    x_train_data = x_data[:int(len(x_data)*0.74+0.5)]\n",
    "    y_train_data = y_data[:int(len(y_data)*0.74+0.5)]\n",
    "    \n",
    "    x_test_data = x_data[int(len(x_data)*0.74+0.5):]\n",
    "    y_test_data = y_data[int(len(y_data)*0.74+0.5):]\n",
    "    \n",
    "    print('time to train model')\n",
    "    \n",
    "    model = SVC(kernel='sigmoid')\n",
    "    history = model.fit(x_train_data,y_train_data)\n",
    "    print(history)\n",
    "    print(type(history))\n",
    "    \n",
    "    y_predict = model.predict(x_test_data)\n",
    "    confusion_m = np.array(confusion_matrix(y_test_data, y_predict,labels = [1,0]))\n",
    "    confusion_m = pd.DataFrame(confusion_m, index=['Went up 3%','Did not go up 3%'],columns=['Predicted up 3%','Predicted not go up 3%'])\n",
    "    confusion_m.to_excel('SVM_Confusion_Matrix.xlsx')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Results\n",
    "Due to SVM creation, training, and testing execution time was incredibly low, I was able to experiment with different kernels and different configuration of features. Out of the kernels tested, linear did absolutely **horribly**, followed by polynomial, RBF, and finally Sigmoid performing the best.\n",
    "\n",
    "The Confusion of the SVM test results for the sigmoid kernel is as follows:\n",
    "<img src=\"img/conf_mat_sig.png\"  width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ignoring the Caveman energy of the row and column titles, I was amazed by the precision of a Sigmoid SVM! In the beginning of the paper I was advocating for a high precision on classification predictions and the Sigmoid SVM defintely did not fail to impress! \n",
    "\n",
    "* I can't wait to test and see how well it performs in conjunction with a logistic regression model (the original plan).\n",
    "\n",
    "* The recall and Accuracy were on the other hand, leave something to be desired. They were only slightly better than randomly saying a stock price would be going up or down the following week. \n",
    "    * However, if the accuracy of 60.8% is honest (which it almost certaintly is **not**), then an almost 11% increase in accuracy from the normal random prediction of a stock price your average Joe would produce is still relatively absolutely phenomenal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
