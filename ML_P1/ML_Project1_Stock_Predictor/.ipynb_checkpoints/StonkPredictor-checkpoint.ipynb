{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copyright 2020 Enea Dodi\n",
    "https://www.apache.org/licenses/LICENSE-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Lets important everything and not worry for later :)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from click._compat import iteritems\n",
    "import requests\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import threading \n",
    "import collections\n",
    "from pandas.tests.frame.test_sort_values_level_as_str import ascending\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project 1: Stock Market Predictors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### By *Enea Dodi* Summer 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "I have designed a 'course' to learn the fundamentals of Machine Learning:\n",
    "\n",
    "1. Complete Google Machine Learning Crash Course \n",
    "2. Simple/fun ML project to have hands on experience\n",
    "3. Andrew Trask's Grokking Deep Learning\n",
    "4. More sophisticated ML project\n",
    "\n",
    "   **option 1**  \n",
    "   5. Pedro Domingo college course for Machine Learning (found on Youtube)  \n",
    "   6. Finish all HW assignments of this course\n",
    "\n",
    "   **option 2**  \n",
    "   5. Aurélien Géron Hand's On Machine Learning edition 2  \n",
    "   6. Complete plenty of exercises in this book.\n",
    "\n",
    "This project is the second on the list: *A simple/fun ML project to have hands on experience*\n",
    "\n",
    "The use of Artifical Intelligence for Stock Market Analysis/Prediction is a very big and competitive field. There are resources and websites which act as communities and API for developing your own algorithms and papers (such as [quandl](https://www.quandl.com/) and [quantopian](https://www.quantopian.com/)) however **my goal for this project is not to develop a industry standard algorithm, but rather have hands on experience with Machine Learning and the tools involved**\n",
    "\n",
    "I will be using TensorFlow, Pandas, Numpy, and Matplotlib for the development of the algorithm, as well as BeautifulSoup to scrape valuable information from stock analysis websites (such as [finviz](https://finviz.com/)). \n",
    "**Big** thanks to Nicolas P. Rougier for [100 Numpy Exercises](https://github.com/rougier/numpy-100) and Alex Riley for [100 Pandas Exercises](https://github.com/ajcr/100-pandas-puzzles) which acted as tutorials for using Numpy and Pandas correctly for Machine Learning Projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preface\n",
    "I would like to establish goals, framing, and overall training route before creation of the Machine Learning algorithms for this project. As a template, I will be using the *Deciding on Machine Learning* outline from Google's Introduction to ML Problem Framing.\n",
    "\n",
    "**1) What should the ML model do?**\n",
    "    - The overall goal of the ML models will be to develop a general classificiation, 'Going up next week' or 'Not going up next week' which works on any Stock Ticker belonging to different sectors, industries, price ranges, and volume \n",
    "    range.\n",
    "**2) What is the ideal outcome?**\n",
    "    - The ideal outcome is to generate a list of stocks which have a 'high probability' of going up in price the \n",
    "    following week so an individual can assess and determine which of the high probability stocks to invest in.\n",
    "**3) How will I know if system is successful or a failure (Sucess / Failure Metrics)**\n",
    "    - Measuring the metrics of success is very simple with stock market predictors. During training and testing, the model will be evaluated on the precision of its classifications for the upcoming week. I will thus know immeditaely, or the following week (as I may decide to evaluate performance on real predictions rather than past price points)\n",
    "    - I will be focusing on precision rather than recall or accuracy. The outcome of investing X amount at 5 companies which all go up 10% is the same as investing X amoumt at 500 companies which all go up 10%. Thus a **low false positive rate** is more important than a **low false negative rate**.\n",
    "    Of course this premise can be challenged. For example if we are investing X amount at 5 companies chosen through a precision omptimized metric, then incorrect labeling, no matter how small of a chance, will result in a larger chunk of cash lost per error compared to investing X amount at 500 companies chosen through a recall optimized metric. Thus I will not attempt to minmax for any strategy, but rather, optimize for precision so far as the batch of positive classifications remains in the low dozens (out of the 1500+ Tickers evaluated). In other words, I will attempt to optimize the precision such as: \n",
    "$$(FPR) * [\\frac{X}{PCC}]  \\leq  [\\frac{X}{TPC}] $$\n",
    "\n",
    "     Where FPR = False Positive Rate ; X = Total Invested Money ; PCC = Positive Clasified Cases ; TPC = True Postiive Cases\n",
    "    \n",
    "    - I will also be displaying the Confusion Matrices, ROC curves, and AUC curves on evaluations. \n",
    "    \n",
    "    \n",
    "    - Finally, stock prices are actually able to do three things: decrease in price, increase in price, or stay the same. The classifier on the other hand only classifies between 'Going up next week' or 'Not going up next week'. Thus I will decide in mostly a subjective matter how to divide up the three classes. \n",
    "        - 'Going up next week' or the positive label will be those stocks which demostrate a 3% or more increase in price in the following week.\n",
    "        - 'Not going up next week' or the negative label will be those stocks which do not demostrate a 3% or more increase in price the following week.\n",
    "            - Upon further development of the models, I may replace the '3%' with the average variance in price of the\n",
    "            specified stock.\n",
    "            \n",
    "**4) Are the Metrics Measurable?**\n",
    "    - All the specified metrics mentioned above are measurable through Stock Market and Data Analysis as they are objective. \n",
    "**5) What failure scenarios are not related to the sucess Matrix?**\n",
    "    - Of course, the features I will be feeding the algorithms are limited. The model must be refurbished frequently as\n",
    "    features such as 'Institutional Holders', 'Income' , 'Sales', and 'Recommendations' are all volatile. Also, the model\n",
    "    is not immune to concept drift, political influence, and unforseen world circumstances. Thus, entire sectors for\n",
    "    example may fall due to some influence not listed on the features and thus these scenarios should not be counted\n",
    "    against the algorithm.\n",
    "    - As a third of the data pushed as features take place during a global pandemic, the model may learn certain \n",
    "    attributes that applying well to the current world circumstances but not towards a 'regular' or 'normal' world\n",
    "    circumstances(which the concepts of may or may not exist as the world is constantly in flux)\n",
    "**6) How will the product use the predictions**\n",
    "     - These positive classifications will then optimally be filtered down through a human interpreter to a hand-full of \n",
    "    Tickers which then the human interpreter can decide to invest in.\n",
    "**7) Where in the architecture should the code live?**\n",
    "    - Every week I shall scrape the closing values on a Thursday and the model should be able to classify which Tickers \n",
    "    will 'Go up next week' before 4pm EST on Friday. There is ample time in this gap, the scraping will be\n",
    "    automated and will only take a small fraction of the day, and the evaluation even less time. Thus there are no\n",
    "    latency requirements\n",
    "\n",
    "### Finally,\n",
    "all my code can be found in the repository. Besides the building of the ML algorithm (which will vastly happen here), all code for Data Extraction and Preperation can be found under the src directory. As there are over a thousand lines of code, it would make no sense to copy paste it all on the presentation. Thus, I will be simply showcasing a few important methods, with **omitted comments** on the Jupyter notebook and the rest of my code can be viewed seperately :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As restrictions I faced with the development of this project included cost, I sought out websites and APIs that were of no cost. While Quandl/Quantopian provides important and consistent data on stocks, free users face many restrictions. Thus I decided to use two sources for data extraction:\n",
    "\n",
    "**NOTE** I was very hyped for the Data Extraction and Preperation. On the Google Machine Learning Crash Course, they stated that the majority of time (some say 80%+) is spend on extraction and preparation of data for Machine Learning models. Thus I wanted to see how I handled that phase. TL;DR I didn't mind it at all!! \n",
    "### 1) Finviz\n",
    "         Finviz is a stock screener and financial visualizer with an awesome free option. It includes a large host\n",
    "         of analytical information (such as Quick Ratio, Average True Range, Short Float, Short Ratio, \n",
    "         Insider Own, etc) as well as a candleled chart with some pattern recogniziton.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Finviz_Graph.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Finviz_table.png\"  width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Yahoo! Finance\n",
    "    Yahoo! finance provides financial news, data, and commentary for the stock market. Yahoo! Primarily I \n",
    "    interacted with Yahoo! Finance through [YFinance](https://pypi.org/project/yfinance/) as Yahoo! decommisioned\n",
    "    their historical data API a couple years back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Yahoo_F_L.png\"  width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finviz offers a large amount of information in the table that could be useful for the ML algorithm to make predictions. However, **A LOT** of this information will offer bias due to them being calculated using performance in the most recent time frame (One week, One month, etc). Thus I will be searching for mostly static values. Ontop of that, unlike the table demostrated above, for the majority of stocks, a lot of information is missing. As an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Avg_table.png\"  width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, I face restrictions on the number of features that should be used on my model. Of course, the fewer the better. Finally, some really good potential features, such as RSI and ATR are momentum indicators and derive from the preivous 14 day values. If I had decided on rescraping/rebuilding my model every day, I would surely use these features. However the purpose of this project is to get my hands dirty in Machine Learning, Data Scraping, and Data Preperation. Thus this will not be necassary for my model. Of course, my model is to be used as a 'filtration' system to recommend the best stocks to look at for a trader. Thus features such as RSI,ATR, and even Short Ratio **should** be used by the actual trader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these restrictions, I had to choose a features that prioritized staticity and were found perfeably on every ticker listed on Finviz. I decided on:\n",
    "   #####  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sector, Industry, Country, Recommendations, Income, and Sales   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YFinance will be used to get the historical chart data for each of the stocks chosen from Finviz. Again, as no specialist, I decided to not rely on Opening, High, Low values. Rather I am interested in how the stock's price looks at **Closing**. However there is a lot of volatility day to day, and this may serve to confuse the model. I am also not interested in short term trades. \n",
    "\n",
    "I thought the larger the time interval per data point, the more consistent to overall trends the price and volume will be. Thus I decided to include in my 'features' (the quotations will make sense towards the end of the project):  \n",
    "#####  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2-Year weekly Closing and Volume values\n",
    "\n",
    "\n",
    "\n",
    "**NOTE** Initally, I had also decided to use Institutional Holders as a Multi-Hot-Encoded feature. This information was also available through YFinance. However, a low percentage of stocks featured these values, and the set of institional holders featured over a thousand unique holders. Thus, it would not be a good feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Data Extraction Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Data Extraction of Finviz, I built a StockScraper class. It utilizes BeautifulSoup,lxml, and requests to scrape information from Finviz.com\n",
    "\n",
    "StockScraper, as one of its class variables, initalizes StockScraperHelper(). Largely, this class was created to avoid overburdening the StockScraper class with methods and to differntiate any general methods that can be reused in other stock scraper classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self):\n",
    "        self.helper = StockScraperHelper()\n",
    "        self.scraped_info = []\n",
    "        self.scraped_tickers = []\n",
    "        self.HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "        #Used to prevent authorization issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main method in the class, get_all_stock_table_information goes on the inital screener page, and will iteratively call get_stock_table_information for each page available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_get_all_stock_table_information(self,url,sector='all',industry='all',country='all',market_cap='all',minPrice=2.50,maxPrice=2000,volume=200000,minimal = True):\n",
    "    \n",
    "    soup = self.get_entire_HTML_page(url)\n",
    "    total = int(soup.find('td',{'class':'count-text'},recursive=True).text.split(' ')[1])\n",
    "    iterations = total // 20\n",
    "\n",
    "    url_extension = 'r='\n",
    "    curr_tickers = 21\n",
    "\n",
    "    l = self.get_stock_table_information(soup, sector, industry, country, market_cap, minPrice, maxPrice, volume)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        next_url = url + url_extension + str(curr_tickers)\n",
    "        print(next_url)\n",
    "        next_soup = self.get_entire_HTML_page(next_url)\n",
    "        curr_tickers += 20\n",
    "        l = l + self.get_stock_table_information(next_soup, sector, industry, country, market_cap, minPrice, maxPrice, volume)\n",
    "\n",
    "    if minimal:\n",
    "        \n",
    "        keys = ('Market Cap', 'Price', 'Change', 'Volume')\n",
    "        for i in l:\n",
    "            for k in keys:\n",
    "                del i[k]\n",
    "\n",
    "    self.scraped_info = l\n",
    "    self.scraped_tickers = self.extract_tickers() `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_get_stock_table_information(self,soup,sector='all',industry='all',country='all',market_cap='all',minPrice=8,maxPrice=1000,volume=200000):\n",
    "    table_rows = soup.find_all('tr',{'class':'table-dark-row-cp'}) + soup.find_all('tr',{'class':'table-light-row-cp'}) \n",
    "    stock_list = []\n",
    "    for r in table_rows:\n",
    "        info = []\n",
    "        for child in r.descendants:\n",
    "            if child.name == 'a':\n",
    "                info.append(child.text)\n",
    "        r_dict = self.helper.row_to_dict(info)\n",
    "        stock_list.append(r_dict)\n",
    "\n",
    "    if sector != 'all':\n",
    "        stock_list = list(filter(lambda x: x['Sector'] == sector,stock_list))\n",
    "    if industry != 'all':\n",
    "        stock_list = list(filter(lambda x: x['Industry'] == industry,stock_list))\n",
    "    if country != 'all':\n",
    "        stock_list = list(filter(lambda x: x['Country'] == country,stock_list))\n",
    "    if market_cap != 'all':\n",
    "        stock_list = list(filter(lambda x: x['Market Cap']  > int(market_cap),stock_list))\n",
    "    stock_list = list(filter(lambda x: (x['Price'] > minPrice) & (x['Price'] < maxPrice),stock_list))\n",
    "    return stock_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations, Income, and Sales are scraped in their seperate method: add_RIS(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ML_P1_data_extraction, I use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_prepare_Stock_Scraper():\n",
    "    ss = StockScraper()\n",
    "    url = 'https://finviz.com/screener.ashx?v=111&'\n",
    "\n",
    "    ss.get_all_stock_table_information(url,market_cap='100000000',minPrice =2,maxPrice = 2000,volume=100000)\n",
    "    print('got table information')\n",
    "    ss.add_RIS()\n",
    "    print('added RIS')\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The total execution time with the listed parameters to build the StockScraper() was:\n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3482.4135 seconds** or roughly 58 minutes for Low Criteria Tickers\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**1588.4088 seconds** or roughly 26.5 minutes for High Criteria Tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the progress, it was time to scrape historical data with YFinance. The code for this is also found in ML_P1_data_extraction.\n",
    "To Showcase the most important method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_and_format_yfinance_data(allTickers,periodt = '1mo',intervalt = '1wk'):\n",
    "    print('period = ', periodt)\n",
    "    print('interval = ', intervalt)\n",
    "    s_tickers = ' '.join(allTickers)\n",
    "    print(s_tickers)\n",
    "    data = yf.download(tickers = allTickers, period = periodt, interval = intervalt, group_by='ticker', auto_adjust=True,threads= True)\n",
    "    rows = data.shape[0]\n",
    "    \n",
    "    queried_data = {}\n",
    "    for t in allTickers:\n",
    "        print(\"Curr ticker: \" + str(t))\n",
    "        queried_data[t] = {}\n",
    "        curr_stock_history = data[t]\n",
    "        for r in range(rows):\n",
    "            curr_date = \"Date: \" + str(curr_stock_history.index[r].date())\n",
    "            queried_data[t][str(curr_date) + ' Close'] = curr_stock_history['Close'].iloc[r]\n",
    "            queried_data[t][str(curr_date) + ' Volume'] = curr_stock_history['Volume'].iloc[r]         \n",
    "            \n",
    "        time.sleep(0.1)\n",
    "            \n",
    "    print(\"finished gathering ticker info from yfinance\")\n",
    "    return queried_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ML_P1_data_extraction file, you will find reminisce of different versions or different applications of this version of query_and_format_yfinance_data. This is due to experimentation. YFinance proved to be very unreliable in its data extraction from Yahoo! Finance. \n",
    "\n",
    "* Firstly, a lot of the methods under the YFinance library were not rigorous. They lacked error checking, and exception handling. To make this code work for all stocks, I had to modify the library quite a bit.\n",
    "\n",
    "\n",
    "* My first implementation featured additional threadding and divison to quicken the process. However, this led YFinance to skipping roughly a third of tickers, even though i introduced locking in my code!\n",
    "\n",
    "\n",
    "* My second implementation is what I ended up using (the code above). However this also featured its own problems! Specifically, it gave a lot of incorrect dates! As an example, this is a snippet demostrating how many null values were at each specified date:\n",
    "\n",
    "> Date: 2020-07-06 Close:      42\n",
    "\n",
    "> Date: 2020-07-07 Close:    1858\n",
    "\n",
    "> Date: 2020-07-13 Close:      37\n",
    "\n",
    "> Date: 2020-07-14 Close:    1858\n",
    "\n",
    "> Date: 2020-07-17 Close:    1859\n",
    "\n",
    "> Date: 2020-07-20 Close:      36\n",
    "\n",
    "> Date: 2020-07-21 Close:    1858\n",
    "\n",
    "> Date: 2020-07-27 Close:      34\n",
    "\n",
    "* As we can see, if a featured week is 2020-07-06 to 2020-07-13, then values that were between these two dates\n",
    "  are all null for all stocks (there were 1859 tickers on the 'High' Criteria list of Tickers.\n",
    "    - This bug was fixed by creating a method that pushes to a list all correct dates and filters the bad ones out for\n",
    "        both Closing and Volume values.\n",
    "        \n",
    "        \n",
    "* My third implementaiton tried to not use the yf.download() method, but rather yf.Ticker() method. While this did avoid getting the incorrect dates, it featured a lot of misisng data (Not as much as the threadding implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The total execution timet to query/scrape information with YFinance was:\n",
    " \n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**745.8681 seconds** or roughly 12.4 minutes for Low Criteria Tickers\n",
    "    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**3895.1087 seconds** or roughly 65 minutes for High Criteria Tickers (featuring Institutional Holder querying and time.sleep of 0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation & Data Preparation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Dangerous Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I planned to only use stocks that have been in the Stock Exhange for atleast the last two years. Sadly, a lot of tickers scraped have been around for less than two years. Thus, these tickers are missing valuable information and do not meet my criteria.\n",
    "\n",
    "I however wanted to use tickers that were salvagable. As in, rather than removing all tickers that have any missing information, I created a function that removes the tickers with the most missing information and writes down the tickers that are missing a substantial amount of information in volume columns, price columns, and categorical columns.\n",
    "\n",
    "The following code did as such:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_information(filename,df,remove_worst = False):\n",
    "    pd.set_option('display.max_rows', len(df))\n",
    "    f = open(filename + '.txt','w')\n",
    "    #Divide up the df into three sections: volume per week, price per week, Non time related values\n",
    "    filter_vpw = [col for col in df if 'Volume' in col]\n",
    "    filter_ppw = [col for col in df if 'Close' in col]\n",
    "    time_rv = filter_vpw + filter_ppw\n",
    "    filter_ntrv = [col for col in df if col not in time_rv]\n",
    "\n",
    "    volume_per_week_df = df[filter_vpw]\n",
    "    price_per_week_df = df[filter_ppw]\n",
    "    categorical_values_df = df[filter_ntrv]\n",
    "    \n",
    "    serv = volume_per_week_df.isnull().sum(axis=1)\n",
    "    serv_m = serv.mean()\n",
    "    serv_std = serv.std()\n",
    "    f.write('average # null per ticker for volume columns in dataframe is: ' + str(serv.mean()) + '\\n')\n",
    "    f.write('std # null per ticker for volume columns in dataframe is: ' + str(serv_std) + '\\n')\n",
    "    serp = price_per_week_df.isnull().sum(axis=1)\n",
    "    serp_m = serp.mean()\n",
    "    serp_std = serv.std()\n",
    "    f.write('average # null per ticker for price columns in dataframe is: ' + str(serp.mean())+ '\\n')\n",
    "    f.write('std # null per ticker for price columns in dataframe is: ' + str(serp_std) + '\\n')\n",
    "    serc = categorical_values_df.isnull().sum(axis=1)\n",
    "    serc_m = serc.mean()\n",
    "    serc_std = serc.std()\n",
    "    f.write('average # null per ticker for categorical columns in dataframe is: ' + str(serc.mean())+ '\\n')\n",
    "    f.write('std #null per ticker for categorical columns in dataframe is: ' + str(serc_std) + '\\n')\n",
    "    \n",
    "    #get subset where the amount of nulls is greater than one standard deviation away from the mean, then sort. \n",
    "    #This'll give me an ide aof which stocks to drop.\n",
    "    serv_outliers = serv[(serv >= serv_m + 1*serv_std)]\n",
    "    serv_outliers.sort_values(ascending=False,inplace=True,na_position='first')\n",
    "    \n",
    "    serp_outliers = serp[(serp >= serp_m + 1*serp_std)]\n",
    "    serp_outliers.sort_values(ascending=False,inplace=True,na_position='first')\n",
    "    \n",
    "    serc_outliers = serc[(serc >= serc_m + 1*serc_std)]\n",
    "    serc_outliers.sort_values(ascending=False,inplace=True,na_position='first')\n",
    "\n",
    "    size = str(len(df.index))\n",
    "    \n",
    "    f.write(\"VOLUME NULL OUTLIERS: \\n\")\n",
    "    f.write(str(serv_outliers.shape[0]) + \"/\" + size + \" are one standard deviation away from average null values per ticker\\n\")\n",
    "    f.write(serv_outliers.to_string())\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "    f.write(\"PRICE NULL OUTLIERS: \\n\")\n",
    "    f.write(str(serp_outliers.shape[0]) +\"/\" + size + \" are one standard deviation away from average null values per ticker\\n\")\n",
    "    f.write(serp_outliers.to_string())\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "    f.write(\"CATEGORICAL NULL OUTLIERS: \\n\")\n",
    "    f.write(str(serc_outliers.shape[0]) + \"/\" + size + \" are one standard deviation away from average null values per ticker\\n\")\n",
    "    f.write(serc_outliers.to_string())\n",
    "    f.write(\"\\n\\n\\n\")\n",
    "    \n",
    "    \n",
    "    .\n",
    "    serv_i = serv_outliers.index\n",
    "    serp_i = serp_outliers.index\n",
    "    serc_i = serc_outliers.index\n",
    "    f.write(\"BAD STOCKS\")\n",
    "    bad_stocks = set(serv_i).intersection(serp_i).intersection(serc_i)\n",
    "    for b in bad_stocks:\n",
    "        f.write(b)\n",
    "        f.write('\\n\\n')\n",
    "    \n",
    "    if remove_worst == True:\n",
    "        for i in serp_i:\n",
    "            df.drop(i,inplace=True)\n",
    "        return df\n",
    "    pd.reset_option('display.max_rows')\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteratively applying this function demostrated a few things:\n",
    "* If a 'Close' column was missing a value at a certain date, then the volume was also missing at the same date.\n",
    "* If a Stock was missing a value at a certain certain date, then the dates were almost entirely before the debut of the Ticker in the Stock Exchange\n",
    "\n",
    "Thus I learned that in my case, I would get practically the same result if I simply created a function that dropped rows if the start date Volume and Price columns were empty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Google Machine Learning Crash Course states that if a feature value appears less than 5 times, then it is not a good feature. I grouped all the tickers in the DataFrame into Sectors and Industry and counted occurences of each.\n",
    "\n",
    "` df_groupby_sector = df.groupby('Sector').count() `\n",
    "\n",
    "` df_groupby_industry = df.groupby('Industry').count() ` \n",
    "    \n",
    "Each sector had plenty of examples, thus I did not have to cut any sectors and their associated tickers.\n",
    "However, there were industries with less than 5 examples. I decided to cut those industries and their associated tickers with the following code: \n",
    "\n",
    "\n",
    "For countries, the majority of tickers belonged in the United States. However, there were tickers that belonged to other countries. Some of them, like Italy and Ukraine had less than 5 occurences. Thus I removed any country alongside their tickers that had less than 5 occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_remove_bad_ticks_by_group(df,occurences,group_n):\n",
    "    g = df.groupby(df[group_n]).count()\n",
    "    \n",
    "    bad_g = list(g[g['Sector'] < occurences].index.values)\n",
    "    print(bad_g)\n",
    "    df_i = df[[group_n]]\n",
    "    for index,row in df_i.iterrows():\n",
    "        if row.values[0] in bad_g:\n",
    "            df.drop(index,inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most features that were scraped from Finviz are features that will be one-hot encoded for the model. These features include:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Sector, Industry, Country, Recommendations**\n",
    "\n",
    "Every stock has a sector, industry, and country value. The recommendations feature has null values. Thus for recommendation I will be adding another OHE feature that tracks 'Has Recommendation Value'\n",
    "\n",
    "All this is handled by the same function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_one_hot_encode(df,c_name,contains_null = False,null_value = None,sparsev=False):\n",
    "    r_df = pd.concat([df,pd.get_dummies(df[c_name],prefix=c_name,sparse=sparsev)],axis=1,sort=False)\n",
    "    \n",
    "    def check_for_null(x):\n",
    "        \n",
    "        if null_value is None:\n",
    "            if pd.isnull(x):\n",
    "                print_global_count() # To see if I miss a null or not.\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        else:\n",
    "            if x is null_value:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "            \n",
    "            \n",
    "    if contains_null == True:\n",
    "        r_df['Contain ' + c_name] = df[c_name].apply(check_for_null)\n",
    "    \n",
    "    r_df.drop([c_name],axis=1,inplace=True)\n",
    "    print('Done one hot encoding: ' , c_name)\n",
    "\n",
    "    reset_global_counter()\n",
    "    return r_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While preparing categorical features was a simple task, preparing non-categorical features proved to be slightly trickier.\n",
    "\n",
    "First I had to deal with missing 'Income' and 'Sales' values. On average, there were substantially more missing 'Income' values than 'Sales' values. I used this fact in my development of inferred 'Income' and 'Sales' values.\n",
    "\n",
    "First I dealt with missing Income values. \n",
    "\n",
    "* A dataframe with tickers with a null income value was initalized.\n",
    "* If for the ticker the sales value was not null:\n",
    "    * Get group of tickers with the same sector and get median value of the ratio of the sector's income and sales values.\n",
    "        * `median_income_div_sales = (df_sect['Income'] / df_sect['Sales']).median()`\n",
    "    * Multiply the sales value with median_income_div_sales\n",
    "* If for the ticker the sales value was null:\n",
    "    * Calculate and place mean income value for the tickers in the same indutsry \n",
    "        * ` m_val_by_industry = in_df_indust.get_group(industry).mean()`\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "Similar logic was used to fill the missing 'Sales' values, but now we knew for sure that every ticker has a 'Income' value thus we can specialize the sales inferred value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My DataFrame is set up so each row represents a ticker where the ticker name is the index, and the columns include all the features.\n",
    "\n",
    "There were features that had to be scaled row wise, and features that had to be scaled column wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row-wise Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date 'Volume' and 'Closing' values had to be scaled row wise. \n",
    "I was on the fence if stock closing values are heteroscedastic. A paper by [JL Sharma](https://www.tandfonline.com/doi/abs/10.1080/096031096334132) provides proof that stock returns are heteroscedastic but no evidence online has to do with prices and volume.\n",
    "\n",
    "Upon reaserch online, many articles demostrated [price](https://www.investopedia.com/articles/investing/102014/lognormal-and-normal-distribution.asp) values of stocks are lognormal\n",
    "\n",
    "Simply put, \n",
    "\n",
    "If the random variable X is log-normally distributed, then Y = ln(X) has a normal distribution.\n",
    "\n",
    "\n",
    "Thus for the closing values, I will be first transforming the values to ln(X) then I will do Standard scale.\n",
    "\n",
    "For volume I initially thought of using MinMax scaling however of fear (probably unreasonable, and this is where my inexperienced of data analysis and ML shines) of using two different scaling mechanisms for likely correlated values is not the best idea. Thus I decided to scale volume the same as I scaled closing values. \n",
    "\n",
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{new} =  \\frac{x-\\mu}{\\sigma}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code used for both Volume and Price is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_log_normal_scale_rows(df):\n",
    "    \n",
    "    for column in df:\n",
    "        df.loc[:,column] = np.log(df[column])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df2 = pd.DataFrame(columns = df.columns,index=df.index,data=scaler.fit_transform(df.values.T).T)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column-wise Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Income' and 'Sales' values had to be scaled column wise. Intutiively I thought using a MinMax Scalar per sector would be best rather than using a single MinMax Scalar for the entire columns of 'Income' and 'Sales'. This very well may have been an error from my part but I believed ticker's incomes and sales should be compared to their sector's incomes and sales. Dividing it up by Industry would stretch the groups too thin and would ruin this feature for the model\n",
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  x_{new} = \\frac{x-x_{min}}{x_{max}-x_{min}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code used for both Income and Sales is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JUPYTER_scale_MinMax_by_Sector(df,s_or_i='Sales'):\n",
    "    g = df.groupby(df['Sector'])\n",
    "    df_section = df[[s_or_i]]\n",
    "    names = g.groups.keys()\n",
    "    scaler = MinMaxScaler()\n",
    "    for n in names:\n",
    "\n",
    "        dfn = g.get_group(n)[[s_or_i]]\n",
    "\n",
    "        dfn_scaled = pd.DataFrame(index=dfn.index,data=scaler.fit_transform(dfn.values))\n",
    "        \n",
    "        df_section[df_section.index.isin(list(dfn.index))] = dfn_scaled\n",
    "\n",
    "        dfn_scaled.to_excel('minmax'+n+'.xlsx')\n",
    "    return df_section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Different Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the current code, I have generated a scaled and one hot encoded Dataframe with features being the columns of the DataFrame and the rows being the examples. \n",
    "\n",
    "The columns (features) consist of : \n",
    "* Sector, Industry, Recommendations and Country One-Hot Encoded\n",
    "* Sales and Income scaled by MinMax\n",
    "* Volume and Date scaled lognormally for involved Dates\n",
    "\n",
    "I have created two datasets:\n",
    "* **2YStockDFHighCriteria** - Features less examples but stocks strating with a higher Market Capital\n",
    "* **2YStockDFLowCriteria** - Features more examples but stocks starting with a lower Market Capital\n",
    "\n",
    "\n",
    "Depending on the needs of the models I will be developing, I wrote a couple of methods to create different forms of these dataset.\n",
    "1. Dataset with Sector removed. (**2YStockDFSR**)\n",
    "2. Dataset with Volume columns removed. (**2YStockDFVR**)\n",
    "3. Dataset with a new feature, Price/Volume, lognormally scaled which replaces the Price and Volume columns (**2YStockDFRatio**)\n",
    "4. Dataset where the time frame is reduced to 6 months resulting in 4 times more examples and ~4 times less Dated features\n",
    "    * This will be applied to the original DataFrame as well as the Dataframe of part 3. (**2YStockDFSplit** , **2YStockDFRatioSplit**)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
