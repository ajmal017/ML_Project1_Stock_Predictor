{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project 1: Stock Market Predictors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### By *Enea Dodi* Summer 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "I have designed a 'course' to learn the fundamentals of Machine Learning:\n",
    "\n",
    "1. Complete Google Machine Learning Crash Course \n",
    "2. Simple/fun ML project to have hands on experience\n",
    "3. Andrew Trask's Grokking Deep Learning\n",
    "4. More sophisticated ML project\n",
    "\n",
    "   **option 1**  \n",
    "   5. Pedro Domingo college course for Machine Learning (found on Youtube)  \n",
    "   6. Finish all HW assignments of this course\n",
    "\n",
    "   **option 2**  \n",
    "   5. Aurélien Géron Hand's On Machine Learning edition 2  \n",
    "   6. Complete plenty of exercises in this book.\n",
    "\n",
    "This project is the second on the list: *A simple/fun ML project to have hands on experience*\n",
    "\n",
    "The use of Artifical Intelligence for Stock Market Analysis/Prediction is a very big and competitive field. There are resources and websites which act as communities and API for developing your own algorithms and papers (such as [quandl](https://www.quandl.com/) and [quantopian](https://www.quantopian.com/)) however **my goal for this project is not to develop a industry standard algorithm, but rather have hands on experience with Machine Learning and the tools involved**\n",
    "\n",
    "I will be using TensorFlow, Pandas, Numpy, and Matplotlib for the development of the algorithm, as well as BeautifulSoup to scrape valuable information from stock analysis websites (such as [finviz](https://finviz.com/)). \n",
    "**Big** thanks to Nicolas P. Rougier for [100 Numpy Exercises](https://github.com/rougier/numpy-100) and Alex Riley for [100 Pandas Exercises](https://github.com/ajcr/100-pandas-puzzles) which acted as tutorials for using Numpy and Pandas correctly for Machine Learning Projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preface\n",
    "I would like to establish goals, framing, and overall training route before creation of the Machine Learning algorithms for this project. As a template, I will be using the *Deciding on Machine Learning* outline from Google's Introduction to ML Problem Framing.\n",
    "\n",
    "**1) What should the ML model do?**\n",
    "    - The overall goal of the ML models will be to develop a general classificiation, 'Going up next week' or 'Not going up next week' which works on any Stock Ticker belonging to different sectors, industries, price ranges, and volume \n",
    "    range.\n",
    "**2) What is the ideal outcome?**\n",
    "    - The ideal outcome is to generate a list of stocks which have a 'high probability' of going up in price the \n",
    "    following week so an individual can assess and determine which of the high probability stocks to invest in.\n",
    "**3) How will I know if system is successful or a failure (Sucess / Failure Metrics)**\n",
    "    - Measuring the metrics of success is very simple with stock market predictors. During training and testing, the model will be evaluated on the precision of its classifications for the upcoming week. I will thus know immeditaely, or the following week (as I may decide to evaluate performance on real predictions rather than past price points)\n",
    "    - I will be focusing on precision rather than recall or accuracy. The outcome of investing X amount at 5 companies which all go up 10% is the same as investing X amoumt at 500 companies which all go up 10%. Thus a **low false positive rate** is more important than a **low false negative rate**.\n",
    "    Of course this premise can be challenged. For example if we are investing X amount at 5 companies chosen through a precision omptimized metric, then incorrect labeling, no matter how small of a chance, will result in a larger chunk of cash lost per error compared to investing X amount at 500 companies chosen through a recall optimized metric. Thus I will not attempt to minmax for any strategy, but rather, optimize for precision so far as the batch of positive classifications remains in the low dozens (out of the 1500+ Tickers evaluated). In other words, I will attempt to optimize the precision such as: \n",
    "$$(FPR) * [\\frac{X}{PCC}]  \\leq  [\\frac{X}{TPC}] $$\n",
    "\n",
    "     Where FPR = False Positive Rate ; X = Total Invested Money ; PCC = Positive Clasified Cases ; TPC = True Postiive Cases\n",
    "    \n",
    "    - I will also be displaying the Confusion Matrices, ROC curves, and AUC curves on evaluations. \n",
    "    \n",
    "    \n",
    "    - Finally, stock prices are actually able to do three things: decrease in price, increase in price, or stay the same. The classifier on the other hand only classifies between 'Going up next week' or 'Not going up next week'. Thus I will decide in mostly a subjective matter how to divide up the three classes. \n",
    "        - 'Going up next week' or the positive label will be those stocks which demostrate a 3% or more increase in price in the following week.\n",
    "        - 'Not going up next week' or the negative label will be those stocks which do not demostrate a 3% or more increase in price the following week.\n",
    "            - Upon further development of the models, I may replace the '3%' with the average variance in price of the\n",
    "            specified stock.\n",
    "            \n",
    "**4) Are the Metrics Measurable?**\n",
    "    - All the specified metrics mentioned above are measurable through Stock Market and Data Analysis as they are objective. \n",
    "**5) What failure scenarios are not related to the sucess Matrix?**\n",
    "    - Of course, the features I will be feeding the algorithms are limited. The model must be refurbished frequently as\n",
    "    features such as 'Institutional Holders', 'Income' , 'Sales', and 'Recommendations' are all volatile. Also, the model\n",
    "    is not immune to concept drift, political influence, and unforseen world circumstances. Thus, entire sectors for\n",
    "    example may fall due to some influence not listed on the features and thus these scenarios should not be counted\n",
    "    against the algorithm.\n",
    "    - As a third of the data pushed as features take place during a global pandemic, the model may learn certain \n",
    "    attributes that applying well to the current world circumstances but not towards a 'regular' or 'normal' world\n",
    "    circumstances(which the concepts of may or may not exist as the world is constantly in flux)\n",
    "**6) How will the product use the predictions**\n",
    "     - These positive classifications will then optimally be filtered down through a human interpreter to a hand-full of \n",
    "    Tickers which then the human interpreter can decide to invest in.\n",
    "**7) Where in the architecture should the code live?**\n",
    "    - Every week I shall scrape the closing values on a Thursday and the model should be able to classify which Tickers \n",
    "    will 'Go up next week' before 4pm EST on Friday. There is ample time in this gap, the scraping will be\n",
    "    automated and will only take a small fraction of the day, and the evaluation even less time. Thus there are no\n",
    "    latency requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
